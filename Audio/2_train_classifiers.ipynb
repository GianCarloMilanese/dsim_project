{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCH=32\n",
    "EPOCHS=50\n",
    "PATIENCE=5\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn_models\n",
    "import data_preparation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "import data_augmentation\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "## No augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdd_dir=\"./recordings/\"\n",
    "our_recs_dir=\"./preprocessed_recs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ./recordings/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db9174228bc48be98ec50da31cccdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading from ./preprocessed_recs/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c6b6eb58b64cf0b5279b1705b4bc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recordings = data_preparation.load_recordings(paths=[fsdd_dir, our_recs_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does input recordings vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010 18262\n"
     ]
    }
   ],
   "source": [
    "min_y = min(map(np.shape, recordings))[0]\n",
    "max_y = max(map(np.shape, recordings))[0]\n",
    "print(min_y, max_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite a huge difference! Let's find out the 10 longest recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18262, 17567, 9015, 8995, 8435, 8281, 8201, 8068, 7755, 7356]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [len(x) for x in recordings]\n",
    "a.sort(reverse=True)\n",
    "a[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get their indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(x) for x in recordings]\n",
    "first_length=18262\n",
    "second_length=17567\n",
    "index_first = a.index(first_length)\n",
    "index_second = a.index(second_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest track is associated with speaker theo, digit 9\n",
      "Second longest track is associated with speaker theo, digit 7\n"
     ]
    }
   ],
   "source": [
    "labels_speakers = data_preparation.load_labels(paths=[fsdd_dir, our_recs_dir], label_type=\"speakers\")\n",
    "labels_digits = data_preparation.load_labels(paths=[fsdd_dir, our_recs_dir])\n",
    "print(\"Longest track is associated with speaker {}, digit {}\".format(labels_speakers[index_first],labels_digits[index_first]))\n",
    "print(\"Second longest track is associated with speaker {}, digit {}\".format(labels_speakers[index_second],labels_digits[index_second]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the problem is with theo, which has 500 recordings, digit 9 and 7, which respectively have 200 recordings. We can safely delete them and saving to pad many thousands of 0s (there will be (18262 - 9015) less zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 2400\n",
      "After: 2398\n"
     ]
    }
   ],
   "source": [
    "max_track_length=9015 # it will be useful later on\n",
    "print(\"Before: {}\".format(len(recordings)))\n",
    "recordings=np.delete(recordings,[index_first, index_second])\n",
    "print(\"After: {}\".format(len(recordings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 2400\n",
      "After: 2398\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: {}\".format(len(labels_speakers)))\n",
    "labels_speakers=np.delete(labels_speakers,[index_first, index_second])\n",
    "print(\"After: {}\".format(len(labels_speakers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 2400\n",
      "After: 2398\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: {}\".format(len(labels_digits)))\n",
    "labels_digits=np.delete(labels_digits,[index_first, index_second])\n",
    "print(\"After: {}\".format(len(labels_digits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now double check to see if everything went well. Now the longest recording will be around 9 K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9015, 8995, 8435, 8281, 8201, 8068, 7755, 7356, 7147, 7038]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [len(x) for x in recordings]\n",
    "a.sort(reverse=True)\n",
    "a[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though variability is reduced, it is still there: for this reason we will pad zeros at start and end of recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_zeros >>>\n",
      "pad_zeros <<<\n"
     ]
    }
   ],
   "source": [
    "pad_recordings = data_preparation.pad_zeros(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now they will have the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9015 9015\n"
     ]
    }
   ],
   "source": [
    "min_y = min(map(np.shape, pad_recordings))[0]\n",
    "max_y = max(map(np.shape, pad_recordings))[0]\n",
    "print(min_y, max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels_speakers, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ale', 'alinda', 'gian', 'jackson', 'khaled', 'nicolas', 'theo',\n",
       "       'yweweler'], dtype='<U8')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 4]),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(counts == np.min(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(labels_speakers == 'ale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def balanced_train_val_test_split(X, y, train_size=0.6):\n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_val = []\n",
    "    y_test = []\n",
    "    # Find out unique values and their occurences\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    # Occurences of the least frequent clas\n",
    "    min_len = np.min(counts)\n",
    "    # How many samples should train, val and test have:\n",
    "    train_freq = int(min_len * train_size)\n",
    "    val_freq = (min_len - train_freq)//2\n",
    "    test_freq = min_len - train_freq - val_freq\n",
    "    print(train_freq, val_freq, test_freq)\n",
    "    for c in unique:\n",
    "        print(c)\n",
    "        current_indexes = np.where(y == c)[0]\n",
    "        np.random.shuffle(current_indexes)\n",
    "        train_indexes = current_indexes[0:train_freq]\n",
    "        val_indexes = current_indexes[train_freq:train_freq+val_freq]\n",
    "        test_indexes = current_indexes[train_freq+val_freq:]\n",
    "        X_train = X_train + [X[i] for i in train_indexes]\n",
    "        y_train = y_train + [y[i] for i in train_indexes]\n",
    "        X_val = X_val + [X[i] for i in val_indexes]\n",
    "        y_val = y_val + [y[i] for i in val_indexes]\n",
    "        X_test = X_test + [X[i] for i in test_indexes]\n",
    "        y_test = y_test + [y[i] for i in test_indexes]\n",
    "    return np.array(X_train), np.array(y_train), np.array(X_val), np.array(y_val), np.array(X_test), np.array(y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 48 48\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "X_train_digits, y_train_digits, X_val_digits, y_val_digits, X_test_digits, y_test_digits = balanced_train_val_test_split(pad_recordings, labels_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "[143 143 143 143 143 143 143 143 143 143]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train_digits, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 20 20\n",
      "ale\n",
      "alinda\n",
      "gian\n",
      "jackson\n",
      "khaled\n",
      "nicolas\n",
      "theo\n",
      "yweweler\n"
     ]
    }
   ],
   "source": [
    "X_train_speakers, y_train_speakers, X_val_speakers, y_val_speakers, X_test_speakers, y_test_speakers = balanced_train_val_test_split(pad_recordings, labels_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ale' 'alinda' 'gian' 'jackson' 'khaled' 'nicolas' 'theo' 'yweweler']\n",
      "[60 60 60 60 60 60 60 60]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train_speakers, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits\n",
    "## Spectrograms - No augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 296 ms, total: 17.7 s\n",
      "Wall time: 9.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_digits_spects = np.array([data_preparation.compute_spectrogram(x) for x in X_train_digits])\n",
    "X_val_digits_spects = np.array([data_preparation.compute_spectrogram(x) for x in X_val_digits])\n",
    "X_test_digits_spects = np.array([data_preparation.compute_spectrogram(x) for x in X_test_digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 429 ms, total: 19.4 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_digits_spects_norm = np.array([data_preparation.compute_spectrogram(x, normalize=True) for x in X_train_digits])\n",
    "X_val_digits_spects_norm = np.array([data_preparation.compute_spectrogram(x, normalize=True) for x in X_val_digits])\n",
    "X_test_digits_spects_norm = np.array([data_preparation.compute_spectrogram(x, normalize=True) for x in X_test_digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_digits_spects.shape\n",
    "X_train_digits_spects_2d = X_train_digits_spects.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 s, sys: 322 ms, total: 27.1 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf1 = SVC(kernel='rbf', class_weight='balanced', gamma=\"auto\")\n",
    "clf1 = clf1.fit(X_train_digits_spects_2d, y_train_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_val_digits_spects.shape\n",
    "X_val_digits_spects_2d = X_val_digits_spects.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.37        48\n",
      "           1       0.83      0.31      0.45        48\n",
      "           2       0.74      0.35      0.48        48\n",
      "           3       0.55      0.33      0.42        48\n",
      "           4       0.26      0.40      0.32        48\n",
      "           5       0.86      0.40      0.54        48\n",
      "           6       0.53      0.21      0.30        48\n",
      "           7       0.67      0.29      0.41        48\n",
      "           8       0.86      0.25      0.39        48\n",
      "           9       0.18      0.92      0.29        48\n",
      "\n",
      "    accuracy                           0.37       480\n",
      "   macro avg       0.65      0.37      0.40       480\n",
      "weighted avg       0.65      0.37      0.40       480\n",
      "\n",
      "CPU times: user 5.38 s, sys: 75.9 ms, total: 5.46 s\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf1.predict(X_val_digits_spects_2d)\n",
    "print(classification_report(y_val_digits, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_digits_spects_norm.shape\n",
    "X_train_digits_spects_norm_2d = X_train_digits_spects_norm.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 188 ms, total: 15.1 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', gamma=\"auto\")\n",
    "clf = clf.fit(X_train_digits_spects_norm_2d, y_train_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_val_digits_spects_norm.shape\n",
    "X_val_digits_spects_norm_2d = X_val_digits_spects_norm.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90        48\n",
      "           1       0.88      0.73      0.80        48\n",
      "           2       0.74      0.90      0.81        48\n",
      "           3       0.93      0.83      0.88        48\n",
      "           4       0.90      0.77      0.83        48\n",
      "           5       0.92      0.75      0.83        48\n",
      "           6       0.76      0.92      0.83        48\n",
      "           7       0.78      0.88      0.82        48\n",
      "           8       0.97      0.75      0.85        48\n",
      "           9       0.81      0.98      0.89        48\n",
      "\n",
      "    accuracy                           0.84       480\n",
      "   macro avg       0.86      0.84      0.84       480\n",
      "weighted avg       0.86      0.84      0.84       480\n",
      "\n",
      "CPU times: user 4.61 s, sys: 55.3 ms, total: 4.67 s\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_val_digits_spects_norm_2d)\n",
    "print(classification_report(y_val_digits, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized spectrogram lead  to better performances.Let's now try with MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 395 ms, total: 22.7 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_digits_mfcc= np.array([data_preparation.mfcc(x, flatten=True) for x in X_train_digits])\n",
    "X_val_digits_mfcc = np.array([data_preparation.mfcc(x, flatten=True) for x in X_val_digits])\n",
    "X_test_digits_mfcc = np.array([data_preparation.mfcc(x, flatten=True) for x in X_test_digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "scaler_normal = StandardScaler()\n",
    "X_train_digits_mfcc_scaled = scaler_normal.fit_transform(X_train_digits_mfcc)\n",
    "X_val_digits_mfcc_scaled =  scaler_normal.transform(X_val_digits_mfcc)\n",
    "X_test_digits_mfcc_scaled =  scaler_normal.transform(X_test_digits_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 38.4 ms, total: 3.48 s\n",
      "Wall time: 3.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SVC(kernel='rbf', class_weight='balanced', gamma=\"auto\")\n",
    "clf = clf.fit(X_train_digits_mfcc_scaled, y_train_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        48\n",
      "           1       0.98      0.96      0.97        48\n",
      "           2       0.96      0.98      0.97        48\n",
      "           3       1.00      0.94      0.97        48\n",
      "           4       1.00      0.96      0.98        48\n",
      "           5       1.00      0.94      0.97        48\n",
      "           6       0.81      1.00      0.90        48\n",
      "           7       0.94      0.94      0.94        48\n",
      "           8       0.98      0.88      0.92        48\n",
      "           9       0.92      0.94      0.93        48\n",
      "\n",
      "    accuracy                           0.95       480\n",
      "   macro avg       0.95      0.95      0.95       480\n",
      "weighted avg       0.95      0.95      0.95       480\n",
      "\n",
      "CPU times: user 1.25 s, sys: 16 ms, total: 1.27 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_val_digits_mfcc_scaled)\n",
    "print(classification_report(y_val_digits, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the best results were obtained my the MFCC representation. Let's use CNN as training models:\n",
    "### CNN\n",
    "#### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 507 ms, total: 24.1 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_digits_mfcc= np.array([data_preparation.mfcc(x, flatten=False) for x in X_train_digits])\n",
    "X_val_digits_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_val_digits])\n",
    "X_test_digits_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_test_digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits_mfcc_nn = X_train_digits_mfcc.reshape(X_train_digits_mfcc.shape[0],\n",
    "                                                     X_train_digits_mfcc.shape[1],\n",
    "                                                     X_train_digits_mfcc.shape[2],\n",
    "                                                     1)\n",
    "X_val_digits_mfcc_nn = X_val_digits_mfcc.reshape(X_val_digits_mfcc.shape[0],\n",
    "                                                 X_val_digits_mfcc.shape[1],\n",
    "                                                 X_val_digits_mfcc.shape[2],\n",
    "                                                 1)\n",
    "X_test_digits_mfcc_nn = X_test_digits_mfcc.reshape(X_test_digits_mfcc.shape[0],\n",
    "                                       X_test_digits_mfcc.shape[1],\n",
    "                                       X_test_digits_mfcc.shape[2],\n",
    "                                       1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train_digits_mfcc_nn.shape[1],\n",
    "               X_train_digits_mfcc_nn.shape[2],\n",
    "               1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,480,234\n",
      "Trainable params: 1,480,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_digits_nn = tf.keras.utils.to_categorical(y_train_digits, 10)\n",
    "y_val_digits_nn = tf.keras.utils.to_categorical(y_val_digits, 10)\n",
    "y_test_digits_nn = tf.keras.utils.to_categorical(y_test_digits, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to train the models, let's start with the simpler one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1430 samples, validate on 480 samples\n",
      "Epoch 1/50\n",
      "1430/1430 [==============================] - 5s 4ms/sample - loss: 2372110.6185 - accuracy: 0.0972 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0902 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0930 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0888 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0958 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0937 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0846 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0965 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0867 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0944 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 11/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0825 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 12/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0965 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 13/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0853 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 14/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0888 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 15/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0909 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 16/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0895 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 17/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0930 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 18/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0993 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 19/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0846 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 20/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0888 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 21/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0923 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 22/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0804 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 23/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0951 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 24/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0881 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 25/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0860 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 26/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0867 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 27/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0874 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 28/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0825 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 29/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0951 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 30/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0818 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 31/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0902 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 32/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 2.3027 - accuracy: 0.0846 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 33/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0853 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 34/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0846 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 35/50\n",
      "1430/1430 [==============================] - 2s 2ms/sample - loss: 2.3027 - accuracy: 0.0874 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "CPU times: user 2min 36s, sys: 1min 18s, total: 3min 55s\n",
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83c1162590>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_digits_mfcc_nn, y_train_digits_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_digits_mfcc_nn, y_val_digits_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        48\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.00      0.00      0.00        48\n",
      "           3       0.00      0.00      0.00        48\n",
      "           4       0.10      1.00      0.18        48\n",
      "           5       0.00      0.00      0.00        48\n",
      "           6       0.00      0.00      0.00        48\n",
      "           7       0.00      0.00      0.00        48\n",
      "           8       0.00      0.00      0.00        48\n",
      "           9       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.10       480\n",
      "   macro avg       0.01      0.10      0.02       480\n",
      "weighted avg       0.01      0.10      0.02       480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kappa/opt/miniconda3/envs/dsim/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "Y_val_nn = np.argmax(y_val_digits_nn,  axis=1)\n",
    "y_pred = model.predict_classes(X_val_digits_mfcc_nn)\n",
    "print(classification_report(Y_val_nn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor results, let's try with the same architecture but with Batch normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 39, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,480,874\n",
      "Trainable params: 1,480,554\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=10, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1430 samples, validate on 480 samples\n",
      "Epoch 1/50\n",
      "1430/1430 [==============================] - 5s 3ms/sample - loss: 1.1045 - accuracy: 0.6629 - val_loss: 1.8985 - val_accuracy: 0.5667\n",
      "Epoch 2/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.5512 - accuracy: 0.8427 - val_loss: 0.8553 - val_accuracy: 0.7208\n",
      "Epoch 3/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.4403 - accuracy: 0.8811 - val_loss: 1.5076 - val_accuracy: 0.5333\n",
      "Epoch 4/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.3453 - accuracy: 0.9238 - val_loss: 0.4714 - val_accuracy: 0.8521\n",
      "Epoch 5/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.3031 - accuracy: 0.9329 - val_loss: 0.3094 - val_accuracy: 0.9292\n",
      "Epoch 6/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.2617 - accuracy: 0.9427 - val_loss: 0.3158 - val_accuracy: 0.9146\n",
      "Epoch 7/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.2400 - accuracy: 0.9357 - val_loss: 0.2428 - val_accuracy: 0.9438\n",
      "Epoch 8/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.2246 - accuracy: 0.9531 - val_loss: 0.2066 - val_accuracy: 0.9563\n",
      "Epoch 9/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.2035 - accuracy: 0.9559 - val_loss: 0.3591 - val_accuracy: 0.8917\n",
      "Epoch 10/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1644 - accuracy: 0.9699 - val_loss: 0.2121 - val_accuracy: 0.9563\n",
      "Epoch 11/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1470 - accuracy: 0.9776 - val_loss: 0.2838 - val_accuracy: 0.9208\n",
      "Epoch 12/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1453 - accuracy: 0.9678 - val_loss: 0.2048 - val_accuracy: 0.9563\n",
      "Epoch 13/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1238 - accuracy: 0.9818 - val_loss: 0.1944 - val_accuracy: 0.9563\n",
      "Epoch 14/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1301 - accuracy: 0.9769 - val_loss: 0.1736 - val_accuracy: 0.9688\n",
      "Epoch 15/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1203 - accuracy: 0.9797 - val_loss: 0.2060 - val_accuracy: 0.9521\n",
      "Epoch 16/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1109 - accuracy: 0.9825 - val_loss: 0.1819 - val_accuracy: 0.9708\n",
      "Epoch 17/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1137 - accuracy: 0.9832 - val_loss: 0.2282 - val_accuracy: 0.9438\n",
      "Epoch 18/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1101 - accuracy: 0.9832 - val_loss: 0.2095 - val_accuracy: 0.9646\n",
      "Epoch 19/50\n",
      "1430/1430 [==============================] - 3s 2ms/sample - loss: 0.1082 - accuracy: 0.9818 - val_loss: 0.2146 - val_accuracy: 0.9500\n",
      "CPU times: user 1min 40s, sys: 51 s, total: 2min 31s\n",
      "Wall time: 52.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83c4d7d450>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_digits_mfcc_nn, y_train_digits_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_digits_mfcc_nn, y_val_digits_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        48\n",
      "           1       0.96      0.98      0.97        48\n",
      "           2       1.00      0.96      0.98        48\n",
      "           3       0.98      0.98      0.98        48\n",
      "           4       0.98      0.98      0.98        48\n",
      "           5       0.98      0.96      0.97        48\n",
      "           6       0.96      0.96      0.96        48\n",
      "           7       0.96      0.94      0.95        48\n",
      "           8       0.94      0.96      0.95        48\n",
      "           9       0.98      0.98      0.98        48\n",
      "\n",
      "    accuracy                           0.97       480\n",
      "   macro avg       0.97      0.97      0.97       480\n",
      "weighted avg       0.97      0.97      0.97       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_val_nn = np.argmax(y_val_digits_nn,  axis=1)\n",
    "y_pred = model.predict_classes(X_val_digits_mfcc_nn)\n",
    "print(classification_report(Y_val_nn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new \"best model-data combo\": CNN + MFCC.\n",
    "\n",
    "Let's now switch to spectrograms. Just for saving some time I will now use batch normalisation for this simpler model and the \"normalised version of the spectrogram representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits_spects_norm_nn = X_train_digits_spects_norm.reshape(X_train_digits_spects_norm.shape[0],\n",
    "                                                     X_train_digits_spects_norm.shape[1],\n",
    "                                                     X_train_digits_spects_norm.shape[2],\n",
    "                                                     1)\n",
    "X_val_digits_spects_norm_nn = X_val_digits_spects_norm.reshape(X_val_digits_spects_norm.shape[0],\n",
    "                                                 X_val_digits_spects_norm.shape[1],\n",
    "                                                 X_val_digits_spects_norm.shape[2],\n",
    "                                                 1)\n",
    "X_test_digits_spects_norm_nn = X_test_digits_spects_norm.reshape(X_test_digits_spects_norm.shape[0],\n",
    "                                       X_test_digits_spects_norm.shape[1],\n",
    "                                       X_test_digits_spects_norm.shape[2],\n",
    "                                       1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train_digits_spects_norm_nn.shape[1],\n",
    "               X_train_digits_spects_norm_nn.shape[2],\n",
    "               1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 127, 56, 32)       160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 127, 56, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 63, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               7225472   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 7,227,562\n",
      "Trainable params: 7,227,242\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=10, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1430 samples, validate on 480 samples\n",
      "Epoch 1/50\n",
      "1430/1430 [==============================] - 11s 8ms/sample - loss: 1.5138 - accuracy: 0.5154 - val_loss: 1.6696 - val_accuracy: 0.6187\n",
      "Epoch 2/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.7686 - accuracy: 0.7573 - val_loss: 1.5799 - val_accuracy: 0.5958\n",
      "Epoch 3/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.4998 - accuracy: 0.8497 - val_loss: 1.3624 - val_accuracy: 0.6854\n",
      "Epoch 4/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.3811 - accuracy: 0.8986 - val_loss: 1.2492 - val_accuracy: 0.6750\n",
      "Epoch 5/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.2992 - accuracy: 0.9161 - val_loss: 1.0286 - val_accuracy: 0.7542\n",
      "Epoch 6/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.2622 - accuracy: 0.9301 - val_loss: 0.9495 - val_accuracy: 0.7312\n",
      "Epoch 7/50\n",
      "1430/1430 [==============================] - 10s 7ms/sample - loss: 0.2109 - accuracy: 0.9545 - val_loss: 0.6405 - val_accuracy: 0.8854\n",
      "Epoch 8/50\n",
      "1430/1430 [==============================] - 10s 7ms/sample - loss: 0.1904 - accuracy: 0.9566 - val_loss: 0.5792 - val_accuracy: 0.8625\n",
      "Epoch 9/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.1756 - accuracy: 0.9594 - val_loss: 0.5206 - val_accuracy: 0.8708\n",
      "Epoch 10/50\n",
      "1430/1430 [==============================] - 9s 7ms/sample - loss: 0.1361 - accuracy: 0.9699 - val_loss: 0.3668 - val_accuracy: 0.9354\n",
      "Epoch 11/50\n",
      "1430/1430 [==============================] - 9s 7ms/sample - loss: 0.1149 - accuracy: 0.9783 - val_loss: 0.3680 - val_accuracy: 0.9250\n",
      "Epoch 12/50\n",
      "1430/1430 [==============================] - 9s 7ms/sample - loss: 0.1048 - accuracy: 0.9874 - val_loss: 0.2834 - val_accuracy: 0.9375\n",
      "Epoch 13/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.1009 - accuracy: 0.9853 - val_loss: 0.2521 - val_accuracy: 0.9375\n",
      "Epoch 14/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0965 - accuracy: 0.9860 - val_loss: 0.2357 - val_accuracy: 0.9458\n",
      "Epoch 15/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0828 - accuracy: 0.9832 - val_loss: 0.2024 - val_accuracy: 0.9625\n",
      "Epoch 16/50\n",
      "1430/1430 [==============================] - 9s 7ms/sample - loss: 0.0697 - accuracy: 0.9902 - val_loss: 0.2034 - val_accuracy: 0.9542\n",
      "Epoch 17/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0656 - accuracy: 0.9888 - val_loss: 0.1889 - val_accuracy: 0.9625\n",
      "Epoch 18/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0592 - accuracy: 0.9916 - val_loss: 0.1805 - val_accuracy: 0.9583\n",
      "Epoch 19/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0607 - accuracy: 0.9923 - val_loss: 0.1704 - val_accuracy: 0.9646\n",
      "Epoch 20/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0579 - accuracy: 0.9951 - val_loss: 0.1661 - val_accuracy: 0.9604\n",
      "Epoch 21/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0433 - accuracy: 0.9958 - val_loss: 0.1593 - val_accuracy: 0.9625\n",
      "Epoch 22/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0410 - accuracy: 0.9965 - val_loss: 0.1848 - val_accuracy: 0.9521\n",
      "Epoch 23/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0475 - accuracy: 0.9909 - val_loss: 0.1704 - val_accuracy: 0.9667\n",
      "Epoch 24/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0547 - accuracy: 0.9874 - val_loss: 0.1647 - val_accuracy: 0.9563\n",
      "Epoch 25/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0438 - accuracy: 0.9944 - val_loss: 0.1580 - val_accuracy: 0.9625\n",
      "Epoch 26/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0394 - accuracy: 0.9937 - val_loss: 0.1596 - val_accuracy: 0.9646\n",
      "Epoch 27/50\n",
      "1430/1430 [==============================] - 10s 7ms/sample - loss: 0.0351 - accuracy: 0.9951 - val_loss: 0.1498 - val_accuracy: 0.9646\n",
      "Epoch 28/50\n",
      "1430/1430 [==============================] - 10s 7ms/sample - loss: 0.0321 - accuracy: 0.9965 - val_loss: 0.1501 - val_accuracy: 0.9646\n",
      "Epoch 29/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0270 - accuracy: 0.9986 - val_loss: 0.1524 - val_accuracy: 0.9625\n",
      "Epoch 30/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0292 - accuracy: 0.9979 - val_loss: 0.1675 - val_accuracy: 0.9583\n",
      "Epoch 31/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9646\n",
      "Epoch 32/50\n",
      "1430/1430 [==============================] - 9s 6ms/sample - loss: 0.0226 - accuracy: 0.9986 - val_loss: 0.1559 - val_accuracy: 0.9646\n",
      "CPU times: user 12min 28s, sys: 1min 34s, total: 14min 3s\n",
      "Wall time: 4min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83edd44ad0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_digits_spects_norm_nn, y_train_digits_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_digits_spects_norm_nn, y_val_digits_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        48\n",
      "           1       0.98      0.92      0.95        48\n",
      "           2       0.96      0.96      0.96        48\n",
      "           3       0.94      0.98      0.96        48\n",
      "           4       0.96      1.00      0.98        48\n",
      "           5       0.94      0.98      0.96        48\n",
      "           6       0.98      0.96      0.97        48\n",
      "           7       1.00      0.94      0.97        48\n",
      "           8       1.00      0.98      0.99        48\n",
      "           9       0.94      0.96      0.95        48\n",
      "\n",
      "    accuracy                           0.96       480\n",
      "   macro avg       0.97      0.96      0.96       480\n",
      "weighted avg       0.97      0.96      0.96       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_val_digits_spects_norm_nn)\n",
    "print(classification_report(Y_val_nn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close results but MFCC is still better.\n",
    "Let's now switch to data augmentation dataset:\n",
    "### Augmentation - MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "Max length: 9015, shape:(17567,)\n",
      "Max length: 9015, shape:(18262,)\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "conversion_done!\n",
      "transform_recordings >>>\n",
      "9015\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "Padding done\n",
      "transform_recordings <<<\n",
      "CPU times: user 6min 34s, sys: 16.4 s, total: 6min 51s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_digit_mfcc, y_train_digit_mfcc, X_val_digit_mfcc, y_val_digit_mfcc, X_test_digit_mfcc, y_test_digit_mfcc = data_preparation.prepare_augmented_recordings(audio_dirs= [fsdd_dir, our_recs_dir],\n",
    "                             y_type= ['digit', 'digit'],\n",
    "                             n_category_test=15,\n",
    "                             include_pitch=True,\n",
    "                             max_length=max_track_length,\n",
    "                             transform_function=\"mfcc\",\n",
    "                             load_stored_augm_recs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_train_val_split(X, y, train_size=0.66):\n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    y_train = []\n",
    "    # Find out unique values and their occurences\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    # Occurences of the least frequent clas\n",
    "    min_len = np.min(counts)\n",
    "    # How many samples should train, val and test have:\n",
    "    train_freq = int(min_len * train_size)\n",
    "    val_freq = min_len - train_freq\n",
    "    print(train_freq, val_freq)\n",
    "    for c in unique:\n",
    "        print(c)\n",
    "        current_indexes = np.where(y == c)[0]\n",
    "        np.random.shuffle(current_indexes)\n",
    "        train_indexes = current_indexes[0:train_freq]\n",
    "        val_indexes = current_indexes[train_freq:train_freq+val_freq]\n",
    "        test_indexes = current_indexes[train_freq+val_freq:]\n",
    "        X_train = X_train + [X[i] for i in train_indexes]\n",
    "        y_train = y_train + [y[i] for i in train_indexes]\n",
    "        X_val = X_val + [X[i] for i in val_indexes]\n",
    "        y_val = y_val + [y[i] for i in val_indexes]\n",
    "    return np.array(X_train), np.array(y_train), np.array(X_val), np.array(y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839 460\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "X_train_digit, y_train_digit, X_val_digit, y_val_digit= balanced_train_val_split(np.concatenate([X_train_digit, X_val_digit]),\n",
    "                         np.concatenate([y_train_digit, y_val_digit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digit_mfcc_nn = X_train_digit.reshape(X_train_digit.shape[0], X_train_digit.shape[1], X_train_digit.shape[2], 1)\n",
    "X_val_digit_mfcc_nn = X_val_digit.reshape(X_val_digit.shape[0], X_val_digit.shape[1], X_val_digit.shape[2], 1)\n",
    "X_test_digit_mfcc_nn = X_test_digit.reshape(X_test_digit.shape[0], X_test_digit.shape[1], X_test_digit.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train_digit_mfcc_nn.shape[1],\n",
    "               X_train_digit_mfcc_nn.shape[2],\n",
    "               1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_digits_nn = tf.keras.utils.to_categorical(y_train_digit, 10)\n",
    "y_val_digits_nn = tf.keras.utils.to_categorical(y_val_digit, 10)\n",
    "y_test_digits_nn = tf.keras.utils.to_categorical(y_test_digit, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 39, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,480,874\n",
      "Trainable params: 1,480,554\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 18390 samples, validate on 4600 samples\n",
      "Epoch 1/50\n",
      "18390/18390 [==============================] - 39s 2ms/sample - loss: 1.2099 - accuracy: 0.5941 - val_loss: 0.9126 - val_accuracy: 0.7115\n",
      "Epoch 2/50\n",
      "18390/18390 [==============================] - 32s 2ms/sample - loss: 0.9483 - accuracy: 0.6916 - val_loss: 0.7523 - val_accuracy: 0.7635\n",
      "Epoch 3/50\n",
      "18390/18390 [==============================] - 34s 2ms/sample - loss: 0.8652 - accuracy: 0.7171 - val_loss: 0.8686 - val_accuracy: 0.7013\n",
      "Epoch 4/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.8033 - accuracy: 0.7399 - val_loss: 0.7352 - val_accuracy: 0.7561\n",
      "Epoch 5/50\n",
      "18390/18390 [==============================] - 32s 2ms/sample - loss: 0.7669 - accuracy: 0.7529 - val_loss: 0.6901 - val_accuracy: 0.7722\n",
      "Epoch 6/50\n",
      "18390/18390 [==============================] - 37s 2ms/sample - loss: 0.7350 - accuracy: 0.7622 - val_loss: 0.6260 - val_accuracy: 0.8041\n",
      "Epoch 7/50\n",
      "18390/18390 [==============================] - 34s 2ms/sample - loss: 0.7350 - accuracy: 0.7614 - val_loss: 0.6462 - val_accuracy: 0.7978\n",
      "Epoch 8/50\n",
      "18390/18390 [==============================] - 33s 2ms/sample - loss: 0.7073 - accuracy: 0.7719 - val_loss: 0.6857 - val_accuracy: 0.7813\n",
      "Epoch 9/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.7128 - accuracy: 0.7703 - val_loss: 0.6475 - val_accuracy: 0.7802\n",
      "Epoch 10/50\n",
      "18390/18390 [==============================] - 28s 2ms/sample - loss: 0.7201 - accuracy: 0.7685 - val_loss: 0.5703 - val_accuracy: 0.8259\n",
      "Epoch 11/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.6991 - accuracy: 0.7725 - val_loss: 0.5445 - val_accuracy: 0.8209\n",
      "Epoch 12/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.6724 - accuracy: 0.7794 - val_loss: 0.5293 - val_accuracy: 0.8267\n",
      "Epoch 13/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.6560 - accuracy: 0.7825 - val_loss: 0.5370 - val_accuracy: 0.8248\n",
      "Epoch 14/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.6786 - accuracy: 0.7775 - val_loss: 0.5587 - val_accuracy: 0.8263\n",
      "Epoch 15/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.7061 - accuracy: 0.7713 - val_loss: 0.5921 - val_accuracy: 0.8102\n",
      "Epoch 16/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.6858 - accuracy: 0.7752 - val_loss: 0.5753 - val_accuracy: 0.8085\n",
      "Epoch 17/50\n",
      "18390/18390 [==============================] - 29s 2ms/sample - loss: 0.6740 - accuracy: 0.7792 - val_loss: 0.5854 - val_accuracy: 0.8107\n",
      "CPU times: user 17min 54s, sys: 11min 1s, total: 28min 56s\n",
      "Wall time: 8min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8279649290>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=10, batch_normalisation=True)\n",
    "model.fit(X_train_digit_mfcc_nn, y_train_digits_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_digit_mfcc_nn, y_val_digits_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       460\n",
      "           1       0.86      0.87      0.86       460\n",
      "           2       0.97      0.67      0.79       460\n",
      "           3       0.66      0.87      0.75       460\n",
      "           4       0.95      0.84      0.89       460\n",
      "           5       0.93      0.82      0.87       460\n",
      "           6       0.81      0.81      0.81       460\n",
      "           7       0.89      0.77      0.83       460\n",
      "           8       0.72      0.85      0.78       460\n",
      "           9       0.73      0.90      0.81       460\n",
      "\n",
      "    accuracy                           0.83      4600\n",
      "   macro avg       0.85      0.83      0.83      4600\n",
      "weighted avg       0.85      0.83      0.83      4600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_val_nn = np.argmax(y_val_digits_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_digit_mfcc_nn)\n",
    "print(classification_report(Y_val_nn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation, in the MFCC scenario, did not lead to any improvement! Let's see what happens in the spectrograms scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms - Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "Max length: 9015, shape:(17567,)\n",
      "Max length: 9015, shape:(18262,)\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "conversion_done!\n",
      "transform_recordings >>>\n",
      "9015\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "Padding done\n",
      "transform_recordings <<<\n",
      "CPU times: user 5min 21s, sys: 15.9 s, total: 5min 37s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_digit, y_train_digit, X_val_digit, y_val_digit, X_test_digit, y_test_digit = data_preparation.prepare_augmented_recordings(audio_dirs= [fsdd_dir, our_recs_dir],\n",
    "                             y_type= ['digit', 'digit'],\n",
    "                             n_category_test=15,\n",
    "                             include_pitch=True,\n",
    "                             max_length=max_track_length,\n",
    "                                                                                                                                  load_stored_augm_recs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517 782\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "X_train_digit, y_train_digit, X_val_digit, y_val_digit= balanced_train_val_split(np.concatenate([X_train_digit, X_val_digit]),\n",
    "                         np.concatenate([y_train_digit, y_val_digit]))\n",
    "X_train_digit_spects_nn = X_train_digit.reshape(X_train_digit.shape[0], X_train_digit.shape[1], X_train_digit.shape[2], 1)\n",
    "X_val_digit_spects_nn = X_val_digit.reshape(X_val_digit.shape[0], X_val_digit.shape[1], X_val_digit.shape[2], 1)\n",
    "X_test_digit_spects_nn = X_test_digit.reshape(X_test_digit.shape[0], X_test_digit.shape[1], X_test_digit.shape[2], 1)\n",
    "input_shape = (X_train_digit_spects_nn.shape[1],\n",
    "               X_train_digit_spects_nn.shape[2],\n",
    "               1)\n",
    "y_train_digits_nn = tf.keras.utils.to_categorical(y_train_digit, 10)\n",
    "y_val_digits_nn = tf.keras.utils.to_categorical(y_val_digit, 10)\n",
    "y_test_digits_nn = tf.keras.utils.to_categorical(y_test_digit, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 127, 56, 32)       160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 127, 56, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 63, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               7225472   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 7,227,562\n",
      "Trainable params: 7,227,242\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15170 samples, validate on 7820 samples\n",
      "Epoch 1/50\n",
      "15170/15170 [==============================] - 114s 8ms/sample - loss: 1.4480 - accuracy: 0.5210 - val_loss: 1.1263 - val_accuracy: 0.6113\n",
      "Epoch 2/50\n",
      "15170/15170 [==============================] - 110s 7ms/sample - loss: 0.9937 - accuracy: 0.6669 - val_loss: 0.9136 - val_accuracy: 0.7083\n",
      "Epoch 3/50\n",
      "15170/15170 [==============================] - 110s 7ms/sample - loss: 0.8320 - accuracy: 0.7218 - val_loss: 0.7926 - val_accuracy: 0.7482\n",
      "Epoch 4/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.7468 - accuracy: 0.7485 - val_loss: 0.6620 - val_accuracy: 0.7900\n",
      "Epoch 5/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.6824 - accuracy: 0.7703 - val_loss: 0.5814 - val_accuracy: 0.8107\n",
      "Epoch 6/50\n",
      "15170/15170 [==============================] - 110s 7ms/sample - loss: 0.6236 - accuracy: 0.7916 - val_loss: 2.7278 - val_accuracy: 0.4009\n",
      "Epoch 7/50\n",
      "15170/15170 [==============================] - 117s 8ms/sample - loss: 0.6497 - accuracy: 0.7811 - val_loss: 6.2028 - val_accuracy: 0.2216\n",
      "Epoch 8/50\n",
      "15170/15170 [==============================] - 156s 10ms/sample - loss: 0.6495 - accuracy: 0.7777 - val_loss: 0.6319 - val_accuracy: 0.7939\n",
      "Epoch 9/50\n",
      "15170/15170 [==============================] - 100s 7ms/sample - loss: 0.5307 - accuracy: 0.8231 - val_loss: 0.5422 - val_accuracy: 0.8219\n",
      "Epoch 10/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.4747 - accuracy: 0.8447 - val_loss: 0.5232 - val_accuracy: 0.8340\n",
      "Epoch 11/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.4288 - accuracy: 0.8554 - val_loss: 0.5222 - val_accuracy: 0.8293\n",
      "Epoch 12/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.3889 - accuracy: 0.8721 - val_loss: 0.5848 - val_accuracy: 0.8160\n",
      "Epoch 13/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.3686 - accuracy: 0.8788 - val_loss: 1.9459 - val_accuracy: 0.6052\n",
      "Epoch 14/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.3509 - accuracy: 0.8847 - val_loss: 102.8202 - val_accuracy: 0.1838\n",
      "Epoch 15/50\n",
      "15170/15170 [==============================] - 101s 7ms/sample - loss: 0.8237 - accuracy: 0.7215 - val_loss: 0.6796 - val_accuracy: 0.7763\n",
      "Epoch 16/50\n",
      "15170/15170 [==============================] - 102s 7ms/sample - loss: 0.6335 - accuracy: 0.7853 - val_loss: 0.6483 - val_accuracy: 0.7788\n",
      "CPU times: user 1h 7min 28s, sys: 9min, total: 1h 16min 28s\n",
      "Wall time: 28min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8252098d50>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=10, batch_normalisation=True)\n",
    "model.fit(X_train_digit_spects_nn, y_train_digits_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_digit_spects_nn, y_val_digits_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       782\n",
      "           1       0.78      0.87      0.82       782\n",
      "           2       0.76      0.81      0.78       782\n",
      "           3       0.81      0.76      0.78       782\n",
      "           4       0.92      0.81      0.86       782\n",
      "           5       0.93      0.83      0.88       782\n",
      "           6       0.77      0.84      0.81       782\n",
      "           7       0.83      0.81      0.82       782\n",
      "           8       0.78      0.87      0.82       782\n",
      "           9       0.92      0.79      0.85       782\n",
      "\n",
      "    accuracy                           0.83      7820\n",
      "   macro avg       0.83      0.83      0.83      7820\n",
      "weighted avg       0.83      0.83      0.83      7820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_val_nn = np.argmax(y_val_digits_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_digit_spects_nn)\n",
    "print(classification_report(Y_val_nn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are worse than the normal scenarios. Let's try to use a \"custom\" CNN architecture, that has less parameters than this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 63, 27, 32)        544       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 12, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               573568    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 608,234\n",
      "Trainable params: 608,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15170 samples, validate on 7820 samples\n",
      "Epoch 1/50\n",
      "15170/15170 [==============================] - 56s 4ms/sample - loss: 1.6493 - accuracy: 0.4044 - val_loss: 6.2094 - val_accuracy: 0.1811\n",
      "Epoch 2/50\n",
      "15170/15170 [==============================] - 55s 4ms/sample - loss: 1.1914 - accuracy: 0.5842 - val_loss: 1.0396 - val_accuracy: 0.6444\n",
      "Epoch 3/50\n",
      "15170/15170 [==============================] - 54s 4ms/sample - loss: 0.9567 - accuracy: 0.6643 - val_loss: 1.8969 - val_accuracy: 0.4948\n",
      "Epoch 4/50\n",
      "15170/15170 [==============================] - 54s 4ms/sample - loss: 0.8463 - accuracy: 0.7030 - val_loss: 0.6997 - val_accuracy: 0.7547\n",
      "Epoch 5/50\n",
      "15170/15170 [==============================] - 54s 4ms/sample - loss: 0.7464 - accuracy: 0.7362 - val_loss: 1.0573 - val_accuracy: 0.6614\n",
      "Epoch 6/50\n",
      "15170/15170 [==============================] - 54s 4ms/sample - loss: 0.6870 - accuracy: 0.7586 - val_loss: 0.7127 - val_accuracy: 0.7564\n",
      "Epoch 7/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.6404 - accuracy: 0.7773 - val_loss: 0.6163 - val_accuracy: 0.7664\n",
      "Epoch 8/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.5956 - accuracy: 0.7890 - val_loss: 1.0771 - val_accuracy: 0.6512\n",
      "Epoch 9/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.5651 - accuracy: 0.8038 - val_loss: 0.8310 - val_accuracy: 0.7243\n",
      "Epoch 10/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.5330 - accuracy: 0.8146 - val_loss: 0.4908 - val_accuracy: 0.8321\n",
      "Epoch 11/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.5034 - accuracy: 0.8204 - val_loss: 0.4558 - val_accuracy: 0.8442\n",
      "Epoch 12/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.4847 - accuracy: 0.8318 - val_loss: 0.5208 - val_accuracy: 0.8137\n",
      "Epoch 13/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.4578 - accuracy: 0.8399 - val_loss: 0.6824 - val_accuracy: 0.7730\n",
      "Epoch 14/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.4528 - accuracy: 0.8415 - val_loss: 0.7125 - val_accuracy: 0.7751\n",
      "Epoch 15/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.4266 - accuracy: 0.8485 - val_loss: 0.4088 - val_accuracy: 0.8587\n",
      "Epoch 16/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.4060 - accuracy: 0.8570 - val_loss: 0.9342 - val_accuracy: 0.7254\n",
      "Epoch 17/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.4170 - accuracy: 0.8527 - val_loss: 0.6245 - val_accuracy: 0.8119\n",
      "Epoch 18/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.3935 - accuracy: 0.8612 - val_loss: 0.3610 - val_accuracy: 0.8770\n",
      "Epoch 19/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.3742 - accuracy: 0.8694 - val_loss: 0.3729 - val_accuracy: 0.8701\n",
      "Epoch 20/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.3652 - accuracy: 0.8708 - val_loss: 0.3664 - val_accuracy: 0.8692\n",
      "Epoch 21/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.3538 - accuracy: 0.8748 - val_loss: 0.4182 - val_accuracy: 0.8543\n",
      "Epoch 22/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.3446 - accuracy: 0.8777 - val_loss: 0.6193 - val_accuracy: 0.8251\n",
      "Epoch 23/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.3428 - accuracy: 0.8798 - val_loss: 0.3371 - val_accuracy: 0.8832\n",
      "Epoch 24/50\n",
      "15170/15170 [==============================] - 53s 4ms/sample - loss: 0.3254 - accuracy: 0.8862 - val_loss: 6.5506 - val_accuracy: 0.3630\n",
      "Epoch 25/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.3861 - accuracy: 0.8649 - val_loss: 0.4067 - val_accuracy: 0.8592\n",
      "Epoch 26/50\n",
      "15170/15170 [==============================] - 53s 3ms/sample - loss: 0.3338 - accuracy: 0.8814 - val_loss: 0.3950 - val_accuracy: 0.8684\n",
      "Epoch 27/50\n",
      "15170/15170 [==============================] - 54s 4ms/sample - loss: 0.3176 - accuracy: 0.8859 - val_loss: 0.3371 - val_accuracy: 0.8822\n",
      "Epoch 28/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.3045 - accuracy: 0.8912 - val_loss: 0.3924 - val_accuracy: 0.8730\n",
      "Epoch 29/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.3021 - accuracy: 0.8894 - val_loss: 0.5035 - val_accuracy: 0.8390\n",
      "Epoch 30/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.2935 - accuracy: 0.8958 - val_loss: 0.3289 - val_accuracy: 0.8848\n",
      "Epoch 31/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.2858 - accuracy: 0.8981 - val_loss: 0.3826 - val_accuracy: 0.8691\n",
      "Epoch 32/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.2761 - accuracy: 0.9022 - val_loss: 0.4200 - val_accuracy: 0.8547\n",
      "Epoch 33/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.2700 - accuracy: 0.9016 - val_loss: 0.3300 - val_accuracy: 0.8864\n",
      "Epoch 34/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.2654 - accuracy: 0.9058 - val_loss: 0.6123 - val_accuracy: 0.8199\n",
      "Epoch 35/50\n",
      "15170/15170 [==============================] - 52s 3ms/sample - loss: 0.2523 - accuracy: 0.9100 - val_loss: 5.4939 - val_accuracy: 0.4340\n",
      "CPU times: user 1h 6min 39s, sys: 27min 9s, total: 1h 33min 48s\n",
      "Wall time: 31min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f825619d310>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = cnn_models.custom_cnn(input_shape=input_shape, num_classes=10)\n",
    "model.fit(X_train_digit_spects_nn, y_train_digits_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_digit_spects_nn, y_val_digits_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       782\n",
      "           1       0.89      0.88      0.89       782\n",
      "           2       0.86      0.88      0.87       782\n",
      "           3       0.83      0.88      0.85       782\n",
      "           4       0.91      0.86      0.89       782\n",
      "           5       0.97      0.85      0.90       782\n",
      "           6       0.95      0.85      0.90       782\n",
      "           7       0.83      0.91      0.87       782\n",
      "           8       0.90      0.89      0.90       782\n",
      "           9       0.80      0.94      0.86       782\n",
      "\n",
      "    accuracy                           0.88      7820\n",
      "   macro avg       0.89      0.88      0.89      7820\n",
      "weighted avg       0.89      0.88      0.89      7820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_val_nn = np.argmax(y_val_digits_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_digit_spects_nn)\n",
    "print(classification_report(Y_val_nn, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 48 48\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "X_train_digits, y_train_digits, X_val_digits, y_val_digits, X_test_digits, y_test_digits = balanced_train_val_test_split(pad_recordings, labels_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits_mfcc_nn = X_train_digits_mfcc.reshape(X_train_digits_mfcc.shape[0],\n",
    "                                                     X_train_digits_mfcc.shape[1],\n",
    "                                                     X_train_digits_mfcc.shape[2],\n",
    "                                                     1)\n",
    "X_val_digits_mfcc_nn = X_val_digits_mfcc.reshape(X_val_digits_mfcc.shape[0],\n",
    "                                                 X_val_digits_mfcc.shape[1],\n",
    "                                                 X_val_digits_mfcc.shape[2],\n",
    "                                                 1)\n",
    "X_test_digits_mfcc_nn = X_test_digits_mfcc.reshape(X_test_digits_mfcc.shape[0],\n",
    "                                       X_test_digits_mfcc.shape[1],\n",
    "                                       X_test_digits_mfcc.shape[2],\n",
    "                                       1)\n",
    "input_shape = (X_train_digits_mfcc_nn.shape[1],\n",
    "               X_train_digits_mfcc_nn.shape[2],\n",
    "               1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits_best = np.concatenate([X_train_digits_mfcc_nn, X_val_digits_mfcc_nn])\n",
    "y_train_digits_best = np.concatenate([y_train_digits, y_val_digits])\n",
    "y_train_digits_best = tf.keras.utils.to_categorical(y_train_digits_best, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 39, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,480,874\n",
      "Trainable params: 1,480,554\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1910 samples\n",
      "Epoch 1/14\n",
      "1910/1910 [==============================] - 7s 3ms/sample - loss: 0.9880 - accuracy: 0.6895\n",
      "Epoch 2/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.4758 - accuracy: 0.8681\n",
      "Epoch 3/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.3489 - accuracy: 0.9084\n",
      "Epoch 4/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.2884 - accuracy: 0.9288\n",
      "Epoch 5/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.2524 - accuracy: 0.9424\n",
      "Epoch 6/14\n",
      "1910/1910 [==============================] - 4s 2ms/sample - loss: 0.2513 - accuracy: 0.9476\n",
      "Epoch 7/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.2090 - accuracy: 0.9550\n",
      "Epoch 8/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.2007 - accuracy: 0.9613\n",
      "Epoch 9/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.1788 - accuracy: 0.9717\n",
      "Epoch 10/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.1729 - accuracy: 0.9607\n",
      "Epoch 11/14\n",
      "1910/1910 [==============================] - 4s 2ms/sample - loss: 0.1628 - accuracy: 0.9712\n",
      "Epoch 12/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.1432 - accuracy: 0.9764\n",
      "Epoch 13/14\n",
      "1910/1910 [==============================] - 4s 2ms/sample - loss: 0.1261 - accuracy: 0.9785\n",
      "Epoch 14/14\n",
      "1910/1910 [==============================] - 3s 2ms/sample - loss: 0.1257 - accuracy: 0.9785\n",
      "CPU times: user 1min 13s, sys: 25.2 s, total: 1min 39s\n",
      "Wall time: 51.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82582a0390>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=10, batch_normalisation=True)\n",
    "model.fit(X_train_digits_best, y_train_digits_best,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=14,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_digits_best = tf.keras.utils.to_categorical(y_test_digits, 10)\n",
    "y_nn = np.argmax(y_test_digits_best, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test_digits_mfcc_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        49\n",
      "           1       0.98      0.98      0.98        49\n",
      "           2       0.98      0.92      0.95        49\n",
      "           3       0.83      0.98      0.90        49\n",
      "           4       0.98      0.96      0.97        49\n",
      "           5       0.98      0.98      0.98        49\n",
      "           6       0.94      0.90      0.92        49\n",
      "           7       0.96      0.96      0.96        48\n",
      "           8       0.98      0.92      0.95        49\n",
      "           9       0.98      0.98      0.98        48\n",
      "\n",
      "    accuracy                           0.95       488\n",
      "   macro avg       0.96      0.95      0.96       488\n",
      "weighted avg       0.96      0.95      0.96       488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_nn, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../best_models/digits.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speakers\n",
    "## Std - MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 466 ms, total: 23.6 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=True) for x in X_train_speakers])\n",
    "X_val_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=True) for x in X_val_speakers])\n",
    "X_test_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=True) for x in X_test_speakers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', class_weight='balanced', gamma=\"auto\")\n",
    "clf = clf.fit(X_train_speakers_mfcc, y_train_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       1.00      0.05      0.10        20\n",
      "      alinda       0.00      0.00      0.00        20\n",
      "        gian       0.00      0.00      0.00        20\n",
      "     jackson       1.00      0.25      0.40        20\n",
      "      khaled       0.14      1.00      0.24        20\n",
      "     nicolas       1.00      0.30      0.46        20\n",
      "        theo       1.00      0.10      0.18        20\n",
      "    yweweler       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.21       160\n",
      "   macro avg       0.52      0.21      0.17       160\n",
      "weighted avg       0.52      0.21      0.17       160\n",
      "\n",
      "CPU times: user 145 ms, sys: 3.42 ms, total: 149 ms\n",
      "Wall time: 152 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kappa/opt/miniconda3/envs/dsim/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_val_speakers_mfcc)\n",
    "print(classification_report(y_val_speakers, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 274 ms, total: 22.5 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_train_speakers])\n",
    "X_val_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_val_speakers])\n",
    "X_test_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_test_speakers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_speakers_mfcc_nn = X_train_speakers_mfcc.reshape(X_train_speakers_mfcc.shape[0],\n",
    "                                                     X_train_speakers_mfcc.shape[1],\n",
    "                                                     X_train_speakers_mfcc.shape[2],\n",
    "                                                     1)\n",
    "X_val_speakers_mfcc_nn = X_val_speakers_mfcc.reshape(X_val_speakers_mfcc.shape[0],\n",
    "                                                 X_val_speakers_mfcc.shape[1],\n",
    "                                                 X_val_speakers_mfcc.shape[2],\n",
    "                                                 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train_speakers_mfcc_nn.shape[1], X_train_speakers_mfcc_nn.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, y_train_speakers_nn, target_names = data_preparation.transform_categorical_y(y_train_speakers)\n",
    "y_val_speakers_nn = enc.transform(y_val_speakers.reshape(-1, 1)).toarray()\n",
    "y_test_speakers_nn = enc.transform(y_test_speakers.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 8)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_speakers_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 1)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 39, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 1,480,616\n",
      "Trainable params: 1,480,296\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 480 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "480/480 [==============================] - 3s 7ms/sample - loss: 1.4323 - accuracy: 0.5417 - val_loss: 20.6215 - val_accuracy: 0.1250\n",
      "Epoch 2/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.5879 - accuracy: 0.8125 - val_loss: 11.4128 - val_accuracy: 0.1625\n",
      "Epoch 3/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.5193 - accuracy: 0.8271 - val_loss: 3.6337 - val_accuracy: 0.2875\n",
      "Epoch 4/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.4204 - accuracy: 0.8750 - val_loss: 4.9752 - val_accuracy: 0.1437\n",
      "Epoch 5/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.3244 - accuracy: 0.9208 - val_loss: 2.4445 - val_accuracy: 0.4750\n",
      "Epoch 6/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.2895 - accuracy: 0.9271 - val_loss: 3.0633 - val_accuracy: 0.3250\n",
      "Epoch 7/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.2305 - accuracy: 0.9375 - val_loss: 0.5754 - val_accuracy: 0.7750\n",
      "Epoch 8/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.1975 - accuracy: 0.9625 - val_loss: 0.5497 - val_accuracy: 0.7937\n",
      "Epoch 9/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.1850 - accuracy: 0.9604 - val_loss: 0.3318 - val_accuracy: 0.8938\n",
      "Epoch 10/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.1677 - accuracy: 0.9729 - val_loss: 0.4037 - val_accuracy: 0.8875\n",
      "Epoch 11/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.1477 - accuracy: 0.9708 - val_loss: 0.6374 - val_accuracy: 0.7625\n",
      "Epoch 12/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.1552 - accuracy: 0.9646 - val_loss: 0.3136 - val_accuracy: 0.8813\n",
      "Epoch 13/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.1126 - accuracy: 0.9812 - val_loss: 0.1857 - val_accuracy: 0.9500\n",
      "Epoch 14/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0958 - accuracy: 0.9896 - val_loss: 0.1566 - val_accuracy: 0.9500\n",
      "Epoch 15/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0933 - accuracy: 0.9875 - val_loss: 0.1323 - val_accuracy: 0.9688\n",
      "Epoch 16/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0970 - accuracy: 0.9875 - val_loss: 0.1779 - val_accuracy: 0.9500\n",
      "Epoch 17/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0755 - accuracy: 0.9958 - val_loss: 0.1239 - val_accuracy: 0.9750\n",
      "Epoch 18/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0685 - accuracy: 0.9937 - val_loss: 0.1107 - val_accuracy: 0.9750\n",
      "Epoch 19/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0767 - accuracy: 0.9917 - val_loss: 0.1199 - val_accuracy: 0.9750\n",
      "Epoch 20/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0616 - accuracy: 0.9979 - val_loss: 0.1034 - val_accuracy: 0.9750\n",
      "Epoch 21/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0749 - accuracy: 0.9937 - val_loss: 0.0886 - val_accuracy: 0.9875\n",
      "Epoch 22/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0596 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9750\n",
      "Epoch 23/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0586 - accuracy: 0.9917 - val_loss: 0.0953 - val_accuracy: 0.9812\n",
      "Epoch 24/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0491 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9625\n",
      "Epoch 25/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0616 - accuracy: 0.9937 - val_loss: 0.1137 - val_accuracy: 0.9750\n",
      "Epoch 26/50\n",
      "480/480 [==============================] - 1s 2ms/sample - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9812\n",
      "CPU times: user 46.9 s, sys: 14 s, total: 1min\n",
      "Wall time: 30.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8261eaa250>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=8, batch_normalisation=True)\n",
    "model.fit(X_train_speakers_mfcc_nn, y_train_speakers_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "         callbacks=[callback],\n",
    "         validation_data=(X_val_speakers_mfcc_nn, y_val_speakers_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get full performances on val set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       1.00      0.95      0.97        20\n",
      "      alinda       1.00      1.00      1.00        20\n",
      "        gian       1.00      0.95      0.97        20\n",
      "     jackson       1.00      1.00      1.00        20\n",
      "      khaled       0.95      1.00      0.98        20\n",
      "     nicolas       1.00      1.00      1.00        20\n",
      "        theo       1.00      1.00      1.00        20\n",
      "    yweweler       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.99       160\n",
      "   macro avg       0.99      0.99      0.99       160\n",
      "weighted avg       0.99      0.99      0.99       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_val_nn = np.argmax(y_val_speakers_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_speakers_mfcc_nn)\n",
    "print(classification_report(Y_val_nn, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent performances! Let's now see what happens with spectrograms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Std - Spects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 275 ms, total: 18 s\n",
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speakers_spects = np.array([data_preparation.compute_spectrogram(x, normalize=True) for x in X_train_speakers])\n",
    "X_val_speakers_spects = np.array([data_preparation.compute_spectrogram(x, normalize=True) for x in X_val_speakers])\n",
    "X_test_speakers_spects = np.array([data_preparation.compute_spectrogram(x, normalize=True) for x in X_test_speakers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_speakers_spects.shape\n",
    "X_train_speakers_spects_2d = X_train_speakers_spects.reshape((nsamples, nx * ny))\n",
    "nsamples, nx, ny = X_val_speakers_spects.shape\n",
    "X_val_speakers_spects_2d = X_val_speakers_spects.reshape((nsamples, nx * ny))\n",
    "nsamples, nx, ny = X_test_speakers_spects.shape\n",
    "X_test_speakers_spects_2d = X_test_speakers_spects.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', class_weight='balanced', gamma=\"auto\")\n",
    "clf = clf.fit(X_train_speakers_spects_2d, y_train_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       1.00      0.75      0.86        20\n",
      "      alinda       1.00      1.00      1.00        20\n",
      "        gian       1.00      1.00      1.00        20\n",
      "     jackson       0.95      1.00      0.98        20\n",
      "      khaled       0.95      0.90      0.92        20\n",
      "     nicolas       0.95      0.95      0.95        20\n",
      "        theo       0.75      0.90      0.82        20\n",
      "    yweweler       0.81      0.85      0.83        20\n",
      "\n",
      "    accuracy                           0.92       160\n",
      "   macro avg       0.93      0.92      0.92       160\n",
      "weighted avg       0.93      0.92      0.92       160\n",
      "\n",
      "CPU times: user 556 ms, sys: 13.8 ms, total: 570 ms\n",
      "Wall time: 597 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf.predict(X_val_speakers_spects_2d)\n",
    "print(classification_report(y_val_speakers, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances are good but not at the level of MFCC: let's use the three diffrent CNN architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_speakers_spects_nn = X_train_speakers_spects.reshape(X_train_speakers_spects.shape[0],\n",
    "                                                     X_train_speakers_spects.shape[1],\n",
    "                                                     X_train_speakers_spects.shape[2],\n",
    "                                                             1)\n",
    "X_val_speakers_spects_nn = X_val_speakers_spects.reshape(X_val_speakers_spects.shape[0],\n",
    "                                                 X_val_speakers_spects.shape[1],\n",
    "                                                 X_val_speakers_spects.shape[2],\n",
    "                                                 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, y_train_speakers_nn, target_names = data_preparation.transform_categorical_y(y_train_speakers)\n",
    "y_val_speakers_nn = enc.transform(y_val_speakers.reshape(-1, 1)).toarray()\n",
    "y_test_speakers_nn = enc.transform(y_test_speakers.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(X_train_speakers_spects_nn.shape[1], X_train_speakers_spects_nn.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 63, 27, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 30, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 5, 64)         32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 80)                30800     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 328       \n",
      "=================================================================\n",
      "Total params: 67,744\n",
      "Trainable params: 67,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.paper_architecture(8, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "480/480 [==============================] - 3s 5ms/sample - loss: 2.1464 - accuracy: 0.1021 - val_loss: 2.0764 - val_accuracy: 0.1125\n",
      "Epoch 2/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0826 - accuracy: 0.1562 - val_loss: 2.0552 - val_accuracy: 0.2250\n",
      "Epoch 3/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0686 - accuracy: 0.1646 - val_loss: 2.0462 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0532 - accuracy: 0.1833 - val_loss: 2.0323 - val_accuracy: 0.2875\n",
      "Epoch 5/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0409 - accuracy: 0.1917 - val_loss: 2.0197 - val_accuracy: 0.3250\n",
      "Epoch 6/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0339 - accuracy: 0.1979 - val_loss: 2.0043 - val_accuracy: 0.3250\n",
      "Epoch 7/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0233 - accuracy: 0.2313 - val_loss: 1.9947 - val_accuracy: 0.3438\n",
      "Epoch 8/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 2.0100 - accuracy: 0.2583 - val_loss: 1.9815 - val_accuracy: 0.3313\n",
      "Epoch 9/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.9916 - accuracy: 0.2521 - val_loss: 1.9583 - val_accuracy: 0.4313\n",
      "Epoch 10/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.9643 - accuracy: 0.2708 - val_loss: 1.9368 - val_accuracy: 0.3812\n",
      "Epoch 11/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.9506 - accuracy: 0.2479 - val_loss: 1.9201 - val_accuracy: 0.3938\n",
      "Epoch 12/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.9233 - accuracy: 0.2917 - val_loss: 1.8927 - val_accuracy: 0.3438\n",
      "Epoch 13/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.9221 - accuracy: 0.2396 - val_loss: 1.8470 - val_accuracy: 0.4062\n",
      "Epoch 14/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.8690 - accuracy: 0.3313 - val_loss: 1.8233 - val_accuracy: 0.4250\n",
      "Epoch 15/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.8499 - accuracy: 0.3375 - val_loss: 1.7868 - val_accuracy: 0.3625\n",
      "Epoch 16/50\n",
      "480/480 [==============================] - 2s 3ms/sample - loss: 1.8209 - accuracy: 0.2937 - val_loss: 1.7267 - val_accuracy: 0.4750\n",
      "Epoch 17/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.8065 - accuracy: 0.3438 - val_loss: 1.7379 - val_accuracy: 0.4125\n",
      "Epoch 18/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.7605 - accuracy: 0.3771 - val_loss: 1.6625 - val_accuracy: 0.4437\n",
      "Epoch 19/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.6779 - accuracy: 0.3938 - val_loss: 1.5981 - val_accuracy: 0.4938\n",
      "Epoch 20/50\n",
      "480/480 [==============================] - 2s 3ms/sample - loss: 1.6854 - accuracy: 0.3708 - val_loss: 1.5806 - val_accuracy: 0.5125\n",
      "Epoch 21/50\n",
      "480/480 [==============================] - 2s 3ms/sample - loss: 1.6066 - accuracy: 0.4062 - val_loss: 1.5389 - val_accuracy: 0.5375\n",
      "Epoch 22/50\n",
      "480/480 [==============================] - 2s 4ms/sample - loss: 1.5696 - accuracy: 0.4479 - val_loss: 1.4741 - val_accuracy: 0.5375\n",
      "Epoch 23/50\n",
      "480/480 [==============================] - 2s 4ms/sample - loss: 1.5337 - accuracy: 0.4292 - val_loss: 1.4377 - val_accuracy: 0.4875\n",
      "Epoch 24/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.5207 - accuracy: 0.3938 - val_loss: 1.3606 - val_accuracy: 0.5625\n",
      "Epoch 25/50\n",
      "480/480 [==============================] - 2s 4ms/sample - loss: 1.4794 - accuracy: 0.4229 - val_loss: 1.6543 - val_accuracy: 0.3000\n",
      "Epoch 26/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.4861 - accuracy: 0.4271 - val_loss: 1.3491 - val_accuracy: 0.5750\n",
      "Epoch 27/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.4425 - accuracy: 0.4583 - val_loss: 1.2112 - val_accuracy: 0.6125\n",
      "Epoch 28/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.3555 - accuracy: 0.4938 - val_loss: 1.1691 - val_accuracy: 0.6375\n",
      "Epoch 29/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.3281 - accuracy: 0.4958 - val_loss: 1.1924 - val_accuracy: 0.5750\n",
      "Epoch 30/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.3326 - accuracy: 0.5083 - val_loss: 1.1080 - val_accuracy: 0.6562\n",
      "Epoch 31/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.2529 - accuracy: 0.5229 - val_loss: 1.0887 - val_accuracy: 0.6750\n",
      "Epoch 32/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.1847 - accuracy: 0.5688 - val_loss: 0.9653 - val_accuracy: 0.7125\n",
      "Epoch 33/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.2260 - accuracy: 0.5417 - val_loss: 0.9648 - val_accuracy: 0.7437\n",
      "Epoch 34/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.1637 - accuracy: 0.5667 - val_loss: 0.9063 - val_accuracy: 0.7250\n",
      "Epoch 35/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.1120 - accuracy: 0.5750 - val_loss: 0.8763 - val_accuracy: 0.7125\n",
      "Epoch 36/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.1745 - accuracy: 0.5646 - val_loss: 0.9438 - val_accuracy: 0.6875\n",
      "Epoch 37/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.0268 - accuracy: 0.6187 - val_loss: 0.8736 - val_accuracy: 0.6938\n",
      "Epoch 38/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.0877 - accuracy: 0.5646 - val_loss: 0.8030 - val_accuracy: 0.7500\n",
      "Epoch 39/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.0422 - accuracy: 0.6396 - val_loss: 0.8884 - val_accuracy: 0.6438\n",
      "Epoch 40/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.9375 - accuracy: 0.6604 - val_loss: 0.7414 - val_accuracy: 0.7688\n",
      "Epoch 41/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.9318 - accuracy: 0.6771 - val_loss: 0.8241 - val_accuracy: 0.7188\n",
      "Epoch 42/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.8967 - accuracy: 0.6708 - val_loss: 0.7224 - val_accuracy: 0.7750\n",
      "Epoch 43/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.9699 - accuracy: 0.6396 - val_loss: 0.7435 - val_accuracy: 0.7437\n",
      "Epoch 44/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.9134 - accuracy: 0.6812 - val_loss: 0.6696 - val_accuracy: 0.7625\n",
      "Epoch 45/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.8388 - accuracy: 0.6771 - val_loss: 0.6595 - val_accuracy: 0.7875\n",
      "Epoch 46/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.8449 - accuracy: 0.6792 - val_loss: 0.6206 - val_accuracy: 0.7812\n",
      "Epoch 47/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.9010 - accuracy: 0.6687 - val_loss: 0.6468 - val_accuracy: 0.8313\n",
      "Epoch 48/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.8573 - accuracy: 0.6979 - val_loss: 0.6484 - val_accuracy: 0.7625\n",
      "Epoch 49/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.7747 - accuracy: 0.7354 - val_loss: 0.7010 - val_accuracy: 0.7375\n",
      "Epoch 50/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 0.7605 - accuracy: 0.7375 - val_loss: 0.5817 - val_accuracy: 0.7937\n",
      "CPU times: user 1min 41s, sys: 50.5 s, total: 2min 32s\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f825e66b210>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speakers_spects_nn, y_train_speakers_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speakers_spects_nn, y_val_speakers_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good accuracy, let's try with the Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 63, 27, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 63, 27, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 30, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 14, 5, 64)         32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 14, 5, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 6, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 80)                30800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 8)                 328       \n",
      "=================================================================\n",
      "Total params: 68,608\n",
      "Trainable params: 68,176\n",
      "Non-trainable params: 432\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.paper_architecture(8, input_shape=input_shape, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "480/480 [==============================] - 4s 8ms/sample - loss: 2.1790 - accuracy: 0.2354 - val_loss: 2.0183 - val_accuracy: 0.1937\n",
      "Epoch 2/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.7509 - accuracy: 0.3167 - val_loss: 2.0059 - val_accuracy: 0.2250\n",
      "Epoch 3/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.5233 - accuracy: 0.4313 - val_loss: 2.0114 - val_accuracy: 0.2250\n",
      "Epoch 4/50\n",
      "480/480 [==============================] - 1s 3ms/sample - loss: 1.3276 - accuracy: 0.5167 - val_loss: 2.0489 - val_accuracy: 0.2313\n",
      "Epoch 5/50\n",
      "480/480 [==============================] - 2s 3ms/sample - loss: 1.1944 - accuracy: 0.5542 - val_loss: 2.1223 - val_accuracy: 0.2188\n",
      "Epoch 6/50\n",
      "480/480 [==============================] - 2s 3ms/sample - loss: 1.0500 - accuracy: 0.6396 - val_loss: 2.2201 - val_accuracy: 0.1813\n",
      "Epoch 7/50\n",
      "480/480 [==============================] - 2s 3ms/sample - loss: 0.9671 - accuracy: 0.6625 - val_loss: 2.2834 - val_accuracy: 0.1688\n",
      "CPU times: user 17.3 s, sys: 8.22 s, total: 25.5 s\n",
      "Wall time: 13.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f826a09b5d0>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speakers_spects_nn, y_train_speakers_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speakers_spects_nn, y_val_speakers_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse performances, let's use the \"simple cnn\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 127, 56, 32)       160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 127, 56, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 63, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               7225472   \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 7,227,304\n",
      "Trainable params: 7,226,984\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.simple_model(num_classes=8, input_shape=input_shape, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "480/480 [==============================] - 6s 12ms/sample - loss: 0.9507 - accuracy: 0.7146 - val_loss: 1.3318 - val_accuracy: 0.7625\n",
      "Epoch 2/50\n",
      "480/480 [==============================] - 3s 6ms/sample - loss: 0.3169 - accuracy: 0.9042 - val_loss: 1.2808 - val_accuracy: 0.8062\n",
      "Epoch 3/50\n",
      "480/480 [==============================] - 3s 7ms/sample - loss: 0.2365 - accuracy: 0.9250 - val_loss: 1.3592 - val_accuracy: 0.6250\n",
      "Epoch 4/50\n",
      "480/480 [==============================] - 3s 6ms/sample - loss: 0.1312 - accuracy: 0.9729 - val_loss: 1.4635 - val_accuracy: 0.4625\n",
      "Epoch 5/50\n",
      "480/480 [==============================] - 3s 6ms/sample - loss: 0.1263 - accuracy: 0.9667 - val_loss: 1.5830 - val_accuracy: 0.3875\n",
      "Epoch 6/50\n",
      "480/480 [==============================] - 3s 7ms/sample - loss: 0.0771 - accuracy: 0.9875 - val_loss: 1.6220 - val_accuracy: 0.3438\n",
      "Epoch 7/50\n",
      "480/480 [==============================] - 3s 7ms/sample - loss: 0.0657 - accuracy: 0.9917 - val_loss: 1.6300 - val_accuracy: 0.3438\n",
      "CPU times: user 52.7 s, sys: 7.44 s, total: 1min\n",
      "Wall time: 24.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8268307bd0>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speakers_spects_nn, y_train_speakers_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speakers_spects_nn, y_val_speakers_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems the best CNN \"Spectrograms\" model, let's evaluate it thoroughly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.80      1.00      0.89        16\n",
      "      alinda       1.00      0.87      0.93        23\n",
      "        gian       0.80      0.89      0.84        18\n",
      "     jackson       0.95      0.73      0.83        26\n",
      "      khaled       0.95      1.00      0.97        19\n",
      "     nicolas       0.30      1.00      0.46         6\n",
      "        theo       1.00      0.61      0.75        33\n",
      "    yweweler       0.65      0.68      0.67        19\n",
      "\n",
      "    accuracy                           0.81       160\n",
      "   macro avg       0.81      0.85      0.79       160\n",
      "weighted avg       0.88      0.81      0.82       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_nn = np.argmax(y_val_speakers_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_speakers_spects_nn)\n",
    "print(classification_report(y_pred, y_nn, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation - MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "Max length: 17000, shape:(17567,)\n",
      "Max length: 17000, shape:(18262,)\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "conversion_done!\n",
      "transform_recordings >>>\n",
      "9015\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "Padding done\n",
      "transform_recordings <<<\n",
      "CPU times: user 4min 17s, sys: 11 s, total: 4min 28s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speaker, y_train_speaker, X_val_speaker, y_val_speaker, X_test_speaker, y_test_speaker = data_preparation.prepare_augmented_recordings(\n",
    "    audio_dirs= [our_recs_dir, fsdd_dir],\n",
    "    y_type= ['speakers_us', 'speakers_default'],\n",
    "    n_category_test=30,\n",
    "    include_pitch=False,\n",
    "    max_length=17000,\n",
    "    transform_function=\"mfcc\",\n",
    "    load_stored_augm_recs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 143\n",
      "ale\n",
      "alinda\n",
      "gian\n",
      "jackson\n",
      "khaled\n",
      "nicolas\n",
      "theo\n",
      "yweweler\n",
      "CPU times: user 52 ms, sys: 35.2 ms, total: 87.2 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speaker, y_train_speaker, X_val_speaker, y_val_speaker =balanced_train_val_split(np.concatenate([X_train_speaker, X_val_speaker]),\n",
    "                         np.concatenate([y_train_speaker, y_val_speaker]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_speaker.shape\n",
    "X_train_speaker_2d = X_train_speaker.reshape((nsamples, nx * ny))\n",
    "nsamples, nx, ny = X_val_speaker.shape\n",
    "X_val_speaker_2d = X_val_speaker.reshape((nsamples, nx * ny))\n",
    "nsamples, nx, ny = X_test_speaker.shape\n",
    "X_test_speaker_2d = X_test_speaker.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_normal = StandardScaler()\n",
    "X_train_speaker_2d = scaler_normal.fit_transform(X_train_speaker_2d)\n",
    "X_val_speaker_2d =  scaler_normal.transform(X_val_speaker_2d)\n",
    "X_test_speaker_2d =  scaler_normal.transform(X_test_speaker_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.28 s, sys: 94.4 ms, total: 7.38 s\n",
      "Wall time: 7.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf_speaker_normal = SVC(kernel='rbf', class_weight='balanced', gamma=\"scale\")\n",
    "clf_speaker_normal.fit(X_train_speaker_2d, y_train_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.87      0.98      0.92       128\n",
      "      alinda       0.98      0.98      0.98       143\n",
      "        gian       0.93      0.96      0.95       138\n",
      "     jackson       0.93      0.97      0.95       137\n",
      "      khaled       0.97      0.80      0.88       173\n",
      "     nicolas       0.97      0.95      0.96       146\n",
      "        theo       0.68      0.82      0.74       119\n",
      "    yweweler       0.85      0.76      0.81       160\n",
      "\n",
      "    accuracy                           0.90      1144\n",
      "   macro avg       0.90      0.90      0.90      1144\n",
      "weighted avg       0.90      0.90      0.90      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_speaker_normal.predict(X_val_speaker_2d)\n",
    "print(classification_report(y_pred, y_val_speaker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, y_train_speaker_nn, target_names = data_preparation.transform_categorical_y(y_train_speaker)\n",
    "y_val_speaker_nn = enc.transform(y_val_speaker.reshape(-1, 1)).toarray()\n",
    "y_test_speaker_nn = enc.transform(y_test_speaker.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_speaker = X_train_speaker.reshape(X_train_speaker.shape[0],\n",
    "                                          X_train_speaker.shape[1],\n",
    "                                          X_train_speaker.shape[2],\n",
    "                                          1)\n",
    "X_val_speaker = X_val_speaker.reshape(X_val_speaker.shape[0],\n",
    "                                      X_val_speaker.shape[1],\n",
    "                                      X_val_speaker.shape[2],\n",
    "                                      1)\n",
    "X_test_speaker = X_test_speaker.reshape(X_test_speaker.shape[0],\n",
    "                                        X_test_speaker.shape[1],\n",
    "                                        X_test_speaker.shape[2],\n",
    "                                        1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 1)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (X_train_speaker.shape[1], X_train_speaker.shape[2], 1)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 39, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 1,480,616\n",
      "Trainable params: 1,480,296\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn_models.simple_model(num_classes=8, input_shape=input_shape, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2216 samples, validate on 1144 samples\n",
      "Epoch 1/50\n",
      "2216/2216 [==============================] - 6s 3ms/sample - loss: 1.1487 - accuracy: 0.6128 - val_loss: 5.7423 - val_accuracy: 0.1958\n",
      "Epoch 2/50\n",
      "2216/2216 [==============================] - 4s 2ms/sample - loss: 0.6110 - accuracy: 0.8055 - val_loss: 1.8334 - val_accuracy: 0.4274\n",
      "Epoch 3/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.5146 - accuracy: 0.8461 - val_loss: 0.5410 - val_accuracy: 0.8033\n",
      "Epoch 4/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.4440 - accuracy: 0.8664 - val_loss: 0.4162 - val_accuracy: 0.8497\n",
      "Epoch 5/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.3657 - accuracy: 0.8872 - val_loss: 0.3411 - val_accuracy: 0.8925\n",
      "Epoch 6/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.3271 - accuracy: 0.9021 - val_loss: 0.3061 - val_accuracy: 0.9047\n",
      "Epoch 7/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.3256 - accuracy: 0.9052 - val_loss: 0.2556 - val_accuracy: 0.9170\n",
      "Epoch 8/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2883 - accuracy: 0.9147 - val_loss: 1.0949 - val_accuracy: 0.6407\n",
      "Epoch 9/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2788 - accuracy: 0.9192 - val_loss: 0.5193 - val_accuracy: 0.8322\n",
      "Epoch 10/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2823 - accuracy: 0.9143 - val_loss: 0.4470 - val_accuracy: 0.8383\n",
      "Epoch 11/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2461 - accuracy: 0.9319 - val_loss: 0.2351 - val_accuracy: 0.9283\n",
      "Epoch 12/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2452 - accuracy: 0.9305 - val_loss: 0.2623 - val_accuracy: 0.9082\n",
      "Epoch 13/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2312 - accuracy: 0.9292 - val_loss: 0.4308 - val_accuracy: 0.8392\n",
      "Epoch 14/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2269 - accuracy: 0.9386 - val_loss: 0.2405 - val_accuracy: 0.9178\n",
      "Epoch 15/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2279 - accuracy: 0.9377 - val_loss: 0.9309 - val_accuracy: 0.7343\n",
      "Epoch 16/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2151 - accuracy: 0.9445 - val_loss: 0.2294 - val_accuracy: 0.9231\n",
      "Epoch 17/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2034 - accuracy: 0.9440 - val_loss: 0.2309 - val_accuracy: 0.9222\n",
      "Epoch 18/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1896 - accuracy: 0.9490 - val_loss: 0.2652 - val_accuracy: 0.9108\n",
      "Epoch 19/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1871 - accuracy: 0.9472 - val_loss: 0.3950 - val_accuracy: 0.8462\n",
      "Epoch 20/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.2025 - accuracy: 0.9422 - val_loss: 0.3469 - val_accuracy: 0.8881\n",
      "Epoch 21/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1850 - accuracy: 0.9486 - val_loss: 0.2207 - val_accuracy: 0.9318\n",
      "Epoch 22/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1795 - accuracy: 0.9495 - val_loss: 0.3724 - val_accuracy: 0.8619\n",
      "Epoch 23/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1914 - accuracy: 0.9468 - val_loss: 0.4484 - val_accuracy: 0.8156\n",
      "Epoch 24/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1973 - accuracy: 0.9422 - val_loss: 0.2283 - val_accuracy: 0.9231\n",
      "Epoch 25/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1995 - accuracy: 0.9449 - val_loss: 0.2696 - val_accuracy: 0.8995\n",
      "Epoch 26/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1738 - accuracy: 0.9558 - val_loss: 0.2019 - val_accuracy: 0.9318\n",
      "Epoch 27/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1676 - accuracy: 0.9504 - val_loss: 0.2033 - val_accuracy: 0.9274\n",
      "Epoch 28/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1554 - accuracy: 0.9571 - val_loss: 0.2493 - val_accuracy: 0.9126\n",
      "Epoch 29/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1688 - accuracy: 0.9531 - val_loss: 0.2598 - val_accuracy: 0.9100\n",
      "Epoch 30/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1625 - accuracy: 0.9517 - val_loss: 0.2000 - val_accuracy: 0.9283\n",
      "Epoch 31/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1386 - accuracy: 0.9571 - val_loss: 0.3623 - val_accuracy: 0.8759\n",
      "Epoch 32/50\n",
      "2216/2216 [==============================] - 6s 3ms/sample - loss: 0.1470 - accuracy: 0.9598 - val_loss: 0.1857 - val_accuracy: 0.9336\n",
      "Epoch 33/50\n",
      "2216/2216 [==============================] - 6s 3ms/sample - loss: 0.1360 - accuracy: 0.9634 - val_loss: 0.2617 - val_accuracy: 0.9038\n",
      "Epoch 34/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1362 - accuracy: 0.9657 - val_loss: 0.1710 - val_accuracy: 0.9327\n",
      "Epoch 35/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1341 - accuracy: 0.9639 - val_loss: 0.1930 - val_accuracy: 0.9362\n",
      "Epoch 36/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1438 - accuracy: 0.9598 - val_loss: 0.2389 - val_accuracy: 0.9196\n",
      "Epoch 37/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1272 - accuracy: 0.9653 - val_loss: 0.2052 - val_accuracy: 0.9240\n",
      "Epoch 38/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1234 - accuracy: 0.9639 - val_loss: 0.2855 - val_accuracy: 0.8951\n",
      "Epoch 39/50\n",
      "2216/2216 [==============================] - 5s 2ms/sample - loss: 0.1368 - accuracy: 0.9616 - val_loss: 0.3030 - val_accuracy: 0.8986\n",
      "CPU times: user 4min 54s, sys: 1min 49s, total: 6min 44s\n",
      "Wall time: 3min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82600db790>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speaker, y_train_speaker_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speaker, y_val_speaker_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.99      0.97      0.98       143\n",
      "      alinda       0.95      1.00      0.97       143\n",
      "        gian       0.98      0.95      0.96       143\n",
      "     jackson       1.00      1.00      1.00       143\n",
      "      khaled       0.99      0.97      0.98       143\n",
      "     nicolas       0.98      0.96      0.97       143\n",
      "        theo       0.79      0.79      0.79       143\n",
      "    yweweler       0.79      0.83      0.81       143\n",
      "\n",
      "    accuracy                           0.93      1144\n",
      "   macro avg       0.93      0.93      0.93      1144\n",
      "weighted avg       0.93      0.93      0.93      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_nn = np.argmax(y_val_speaker_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_speaker)\n",
    "print(classification_report(y_nn, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation - Spects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "split_and_augment_dataset >>>\n",
      "enrich_dataset>>>\n",
      "Max length: 17000, shape:(17567,)\n",
      "Max length: 17000, shape:(18262,)\n",
      "enrich_dataset <<<\n",
      "split_and_augment_dataset <<<\n",
      "conversion_done!\n",
      "transform_recordings >>>\n",
      "9015\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "pad_zeros >>>\n",
      "pad_zeros <<<\n",
      "Padding done\n",
      "transform_recordings <<<\n",
      "CPU times: user 3min 21s, sys: 7.86 s, total: 3min 29s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speaker, y_train_speaker, X_val_speaker, y_val_speaker, X_test_speaker, y_test_speaker = data_preparation.prepare_augmented_recordings(\n",
    "    audio_dirs= [our_recs_dir, fsdd_dir],\n",
    "    y_type= ['speakers_us', 'speakers_default'],\n",
    "    n_category_test=30,\n",
    "    include_pitch=False,\n",
    "    max_length=17000,\n",
    "    transform_function=\"spectrogram\",\n",
    "    load_stored_augm_recs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 143\n",
      "ale\n",
      "alinda\n",
      "gian\n",
      "jackson\n",
      "khaled\n",
      "nicolas\n",
      "theo\n",
      "yweweler\n",
      "CPU times: user 237 ms, sys: 271 ms, total: 507 ms\n",
      "Wall time: 633 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speaker, y_train_speaker, X_val_speaker, y_val_speaker =balanced_train_val_split(np.concatenate([X_train_speaker, X_val_speaker]),\n",
    "                         np.concatenate([y_train_speaker, y_val_speaker]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_speaker.shape\n",
    "X_train_speaker_2d = X_train_speaker.reshape((nsamples, nx * ny))\n",
    "nsamples, nx, ny = X_val_speaker.shape\n",
    "X_val_speaker_2d = X_val_speaker.reshape((nsamples, nx * ny))\n",
    "nsamples, nx, ny = X_test_speaker.shape\n",
    "X_test_speaker_2d = X_test_speaker.reshape((nsamples, nx * ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 195 ms, total: 25.5 s\n",
      "Wall time: 26.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf_speaker = SVC(kernel='rbf', class_weight='balanced', gamma=\"scale\")\n",
    "clf_speaker.fit(X_train_speaker_2d, y_train_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.94      0.97      0.96       139\n",
      "      alinda       0.99      0.97      0.98       146\n",
      "        gian       0.96      0.97      0.96       141\n",
      "     jackson       0.96      0.98      0.97       140\n",
      "      khaled       0.96      0.90      0.93       153\n",
      "     nicolas       0.95      0.94      0.94       145\n",
      "        theo       0.61      0.71      0.65       123\n",
      "    yweweler       0.78      0.71      0.75       157\n",
      "\n",
      "    accuracy                           0.89      1144\n",
      "   macro avg       0.89      0.89      0.89      1144\n",
      "weighted avg       0.90      0.89      0.89      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_speaker.predict(X_val_speaker_2d)\n",
    "print(classification_report(y_pred, y_val_speaker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, y_train_speaker_nn, target_names = data_preparation.transform_categorical_y(y_train_speaker)\n",
    "y_val_speaker_nn = enc.transform(y_val_speaker.reshape(-1, 1)).toarray()\n",
    "y_test_speaker_nn = enc.transform(y_test_speaker.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_speaker = X_train_speaker.reshape(X_train_speaker.shape[0],\n",
    "                                          X_train_speaker.shape[1],\n",
    "                                          X_train_speaker.shape[2],\n",
    "                                          1)\n",
    "X_val_speaker = X_val_speaker.reshape(X_val_speaker.shape[0],\n",
    "                                      X_val_speaker.shape[1],\n",
    "                                      X_val_speaker.shape[2],\n",
    "                                      1)\n",
    "X_test_speaker = X_test_speaker.reshape(X_test_speaker.shape[0],\n",
    "                                        X_test_speaker.shape[1],\n",
    "                                        X_test_speaker.shape[2],\n",
    "                                        1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 128, 57, 1)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_speaker.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 127, 56, 32)       160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 127, 56, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 63, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               7225472   \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 7,227,304\n",
      "Trainable params: 7,226,984\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train_speaker.shape[1], X_train_speaker.shape[2], 1)\n",
    "model = cnn_models.simple_model(num_classes=8, input_shape=input_shape, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2216 samples, validate on 1144 samples\n",
      "Epoch 1/50\n",
      "2216/2216 [==============================] - 20s 9ms/sample - loss: 1.1160 - accuracy: 0.6164 - val_loss: 1.5897 - val_accuracy: 0.4886\n",
      "Epoch 2/50\n",
      "2216/2216 [==============================] - 16s 7ms/sample - loss: 0.5296 - accuracy: 0.8195 - val_loss: 1.7306 - val_accuracy: 0.3094\n",
      "Epoch 3/50\n",
      "2216/2216 [==============================] - 17s 7ms/sample - loss: 0.3786 - accuracy: 0.8782 - val_loss: 1.4937 - val_accuracy: 0.5105\n",
      "Epoch 4/50\n",
      "2216/2216 [==============================] - 17s 8ms/sample - loss: 0.2618 - accuracy: 0.9233 - val_loss: 0.9665 - val_accuracy: 0.6801\n",
      "Epoch 5/50\n",
      "2216/2216 [==============================] - 20s 9ms/sample - loss: 0.1909 - accuracy: 0.9472 - val_loss: 0.7598 - val_accuracy: 0.7177\n",
      "Epoch 6/50\n",
      "2216/2216 [==============================] - 17s 8ms/sample - loss: 0.1504 - accuracy: 0.9598 - val_loss: 0.4898 - val_accuracy: 0.8374\n",
      "Epoch 7/50\n",
      "2216/2216 [==============================] - 15s 7ms/sample - loss: 0.1458 - accuracy: 0.9621 - val_loss: 0.3731 - val_accuracy: 0.8759\n",
      "Epoch 8/50\n",
      "2216/2216 [==============================] - 16s 7ms/sample - loss: 0.1054 - accuracy: 0.9761 - val_loss: 0.8732 - val_accuracy: 0.7229\n",
      "Epoch 9/50\n",
      "2216/2216 [==============================] - 17s 8ms/sample - loss: 0.1120 - accuracy: 0.9779 - val_loss: 0.2406 - val_accuracy: 0.9205\n",
      "Epoch 10/50\n",
      "2216/2216 [==============================] - 35s 16ms/sample - loss: 0.0885 - accuracy: 0.9815 - val_loss: 0.3799 - val_accuracy: 0.8628\n",
      "Epoch 11/50\n",
      "2216/2216 [==============================] - 15s 7ms/sample - loss: 0.0768 - accuracy: 0.9869 - val_loss: 0.2509 - val_accuracy: 0.9231\n",
      "Epoch 12/50\n",
      "2216/2216 [==============================] - 17s 8ms/sample - loss: 0.0566 - accuracy: 0.9923 - val_loss: 0.3964 - val_accuracy: 0.8645\n",
      "Epoch 13/50\n",
      "2216/2216 [==============================] - 16s 7ms/sample - loss: 0.0562 - accuracy: 0.9887 - val_loss: 0.2016 - val_accuracy: 0.9327\n",
      "Epoch 14/50\n",
      "2216/2216 [==============================] - 15s 7ms/sample - loss: 0.0433 - accuracy: 0.9959 - val_loss: 0.3528 - val_accuracy: 0.8890\n",
      "Epoch 15/50\n",
      "2216/2216 [==============================] - 15s 7ms/sample - loss: 0.0502 - accuracy: 0.9919 - val_loss: 0.2025 - val_accuracy: 0.9327\n",
      "Epoch 16/50\n",
      "2216/2216 [==============================] - 17s 8ms/sample - loss: 0.0391 - accuracy: 0.9964 - val_loss: 0.2168 - val_accuracy: 0.9309\n",
      "Epoch 17/50\n",
      "2216/2216 [==============================] - 15s 7ms/sample - loss: 0.0293 - accuracy: 0.9982 - val_loss: 0.2046 - val_accuracy: 0.9301\n",
      "Epoch 18/50\n",
      "2216/2216 [==============================] - 15s 7ms/sample - loss: 0.0304 - accuracy: 0.9977 - val_loss: 0.2157 - val_accuracy: 0.9274\n",
      "CPU times: user 11min 17s, sys: 1min 20s, total: 12min 38s\n",
      "Wall time: 5min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82702f2050>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speaker, y_train_speaker_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speaker, y_val_speaker_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.97      0.99      0.98       140\n",
      "      alinda       1.00      0.97      0.99       147\n",
      "        gian       0.98      0.97      0.97       145\n",
      "     jackson       0.99      1.00      0.99       141\n",
      "      khaled       1.00      0.93      0.96       154\n",
      "     nicolas       0.94      0.99      0.96       135\n",
      "        theo       0.83      0.77      0.80       153\n",
      "    yweweler       0.76      0.84      0.80       129\n",
      "\n",
      "    accuracy                           0.93      1144\n",
      "   macro avg       0.93      0.93      0.93      1144\n",
      "weighted avg       0.93      0.93      0.93      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_nn = np.argmax(y_val_speaker_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_speaker)\n",
    "print(classification_report(y_pred, y_nn, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_26 (Conv2D)           (None, 63, 27, 32)        544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 63, 27, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 30, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 5, 64)         32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 14, 5, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 6, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 80)                30800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 8)                 328       \n",
      "=================================================================\n",
      "Total params: 68,608\n",
      "Trainable params: 68,176\n",
      "Non-trainable params: 432\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train_speaker.shape[1], X_train_speaker.shape[2], 1)\n",
    "model = cnn_models.paper_architecture(num_classes=8, input_shape=input_shape, batch_normalisation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2216 samples, validate on 1144 samples\n",
      "Epoch 1/50\n",
      "2216/2216 [==============================] - 11s 5ms/sample - loss: 1.9787 - accuracy: 0.2793 - val_loss: 1.8657 - val_accuracy: 0.2587\n",
      "Epoch 2/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 1.4775 - accuracy: 0.4377 - val_loss: 1.7726 - val_accuracy: 0.2710\n",
      "Epoch 3/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 1.2675 - accuracy: 0.5153 - val_loss: 1.6768 - val_accuracy: 0.2841\n",
      "Epoch 4/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 1.1064 - accuracy: 0.5803 - val_loss: 1.3621 - val_accuracy: 0.4764\n",
      "Epoch 5/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 1.0143 - accuracy: 0.6209 - val_loss: 0.9655 - val_accuracy: 0.6521\n",
      "Epoch 6/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.8762 - accuracy: 0.6900 - val_loss: 0.8608 - val_accuracy: 0.7142\n",
      "Epoch 7/50\n",
      "2216/2216 [==============================] - 10s 4ms/sample - loss: 0.7927 - accuracy: 0.7261 - val_loss: 0.8412 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "2216/2216 [==============================] - 9s 4ms/sample - loss: 0.7003 - accuracy: 0.7577 - val_loss: 0.6594 - val_accuracy: 0.7902\n",
      "Epoch 9/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.6502 - accuracy: 0.7667 - val_loss: 0.5663 - val_accuracy: 0.8234\n",
      "Epoch 10/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.6040 - accuracy: 0.7888 - val_loss: 2.0991 - val_accuracy: 0.4056\n",
      "Epoch 11/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.5748 - accuracy: 0.8132 - val_loss: 0.4992 - val_accuracy: 0.8278\n",
      "Epoch 12/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.5198 - accuracy: 0.8186 - val_loss: 2.1357 - val_accuracy: 0.4624\n",
      "Epoch 13/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.4848 - accuracy: 0.8434 - val_loss: 1.1193 - val_accuracy: 0.5787\n",
      "Epoch 14/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.4653 - accuracy: 0.8412 - val_loss: 0.5462 - val_accuracy: 0.7876\n",
      "Epoch 15/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.4629 - accuracy: 0.8240 - val_loss: 0.3495 - val_accuracy: 0.9030\n",
      "Epoch 16/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.4309 - accuracy: 0.8583 - val_loss: 0.3987 - val_accuracy: 0.8698\n",
      "Epoch 17/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.3917 - accuracy: 0.8610 - val_loss: 0.3768 - val_accuracy: 0.8759\n",
      "Epoch 18/50\n",
      "2216/2216 [==============================] - 6s 3ms/sample - loss: 0.3805 - accuracy: 0.8669 - val_loss: 0.3878 - val_accuracy: 0.8802\n",
      "Epoch 19/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.3638 - accuracy: 0.8791 - val_loss: 0.4602 - val_accuracy: 0.8348\n",
      "Epoch 20/50\n",
      "2216/2216 [==============================] - 7s 3ms/sample - loss: 0.3528 - accuracy: 0.8795 - val_loss: 0.4542 - val_accuracy: 0.8505\n",
      "CPU times: user 3min 19s, sys: 1min 53s, total: 5min 12s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8258a90150>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speaker, y_train_speaker_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speaker, y_val_speaker_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.95      0.99      0.97       137\n",
      "      alinda       0.95      0.98      0.96       139\n",
      "        gian       0.97      0.92      0.94       150\n",
      "     jackson       0.97      0.96      0.97       145\n",
      "      khaled       0.94      0.97      0.95       138\n",
      "     nicolas       0.97      0.93      0.95       148\n",
      "        theo       0.76      0.71      0.73       152\n",
      "    yweweler       0.73      0.77      0.75       135\n",
      "\n",
      "    accuracy                           0.90      1144\n",
      "   macro avg       0.90      0.90      0.90      1144\n",
      "weighted avg       0.90      0.90      0.90      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_nn = np.argmax(y_val_speaker_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_speaker)\n",
    "print(classification_report(y_pred, y_nn, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 63, 27, 32)        544       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 30, 12, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 14, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               573568    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 607,976\n",
      "Trainable params: 607,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train_speaker.shape[1], X_train_speaker.shape[2], 1)\n",
    "model = cnn_models.custom_cnn(num_classes=8, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2216 samples, validate on 1144 samples\n",
      "Epoch 1/50\n",
      "2216/2216 [==============================] - 11s 5ms/sample - loss: 1.9587 - accuracy: 0.2306 - val_loss: 1.8773 - val_accuracy: 0.2552\n",
      "Epoch 2/50\n",
      "2216/2216 [==============================] - 9s 4ms/sample - loss: 1.7186 - accuracy: 0.3461 - val_loss: 1.6457 - val_accuracy: 0.3733\n",
      "Epoch 3/50\n",
      "2216/2216 [==============================] - 10s 4ms/sample - loss: 1.5017 - accuracy: 0.4328 - val_loss: 2.7113 - val_accuracy: 0.1871\n",
      "Epoch 4/50\n",
      "2216/2216 [==============================] - 10s 4ms/sample - loss: 1.3166 - accuracy: 0.5190 - val_loss: 1.5509 - val_accuracy: 0.5192\n",
      "Epoch 5/50\n",
      "2216/2216 [==============================] - 9s 4ms/sample - loss: 1.1012 - accuracy: 0.6115 - val_loss: 1.0996 - val_accuracy: 0.5778\n",
      "Epoch 6/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.9170 - accuracy: 0.6683 - val_loss: 0.7573 - val_accuracy: 0.7133\n",
      "Epoch 7/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.7641 - accuracy: 0.7252 - val_loss: 0.7608 - val_accuracy: 0.7220\n",
      "Epoch 8/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.6427 - accuracy: 0.7757 - val_loss: 0.8225 - val_accuracy: 0.6809\n",
      "Epoch 9/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.5282 - accuracy: 0.8168 - val_loss: 0.6167 - val_accuracy: 0.7579\n",
      "Epoch 10/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.4883 - accuracy: 0.8231 - val_loss: 0.9398 - val_accuracy: 0.6503\n",
      "Epoch 11/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.4259 - accuracy: 0.8502 - val_loss: 0.3402 - val_accuracy: 0.8767\n",
      "Epoch 12/50\n",
      "2216/2216 [==============================] - 9s 4ms/sample - loss: 0.3800 - accuracy: 0.8700 - val_loss: 0.6520 - val_accuracy: 0.7273\n",
      "Epoch 13/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.3421 - accuracy: 0.8741 - val_loss: 0.3441 - val_accuracy: 0.8628\n",
      "Epoch 14/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.3406 - accuracy: 0.8800 - val_loss: 0.4788 - val_accuracy: 0.8033\n",
      "Epoch 15/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.2838 - accuracy: 0.8926 - val_loss: 0.4930 - val_accuracy: 0.8304\n",
      "Epoch 16/50\n",
      "2216/2216 [==============================] - 8s 4ms/sample - loss: 0.2390 - accuracy: 0.9120 - val_loss: 0.3978 - val_accuracy: 0.8549\n",
      "CPU times: user 4min 23s, sys: 1min 14s, total: 5min 38s\n",
      "Wall time: 2min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f826d5263d0>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_speaker, y_train_speaker_nn,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(X_val_speaker, y_val_speaker_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.92      0.96      0.94       137\n",
      "      alinda       0.90      0.99      0.95       130\n",
      "        gian       0.96      0.94      0.95       146\n",
      "     jackson       0.90      0.96      0.93       134\n",
      "      khaled       0.89      0.93      0.91       136\n",
      "     nicolas       0.94      0.83      0.88       163\n",
      "        theo       0.86      0.63      0.73       194\n",
      "    yweweler       0.64      0.88      0.74       104\n",
      "\n",
      "    accuracy                           0.88      1144\n",
      "   macro avg       0.88      0.89      0.88      1144\n",
      "weighted avg       0.88      0.88      0.88      1144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_nn = np.argmax(y_val_speaker_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_val_speaker)\n",
    "print(classification_report(y_pred, y_nn, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 469 ms, total: 23.8 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_train_speakers])\n",
    "X_val_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_val_speakers])\n",
    "X_test_speakers_mfcc = np.array([data_preparation.mfcc(x, flatten=False) for x in X_test_speakers])\n",
    "X_train_speakers_mfcc_nn = X_train_speakers_mfcc.reshape(X_train_speakers_mfcc.shape[0],\n",
    "                                                     X_train_speakers_mfcc.shape[1],\n",
    "                                                     X_train_speakers_mfcc.shape[2],\n",
    "                                                     1)\n",
    "X_val_speakers_mfcc_nn = X_val_speakers_mfcc.reshape(X_val_speakers_mfcc.shape[0],\n",
    "                                                 X_val_speakers_mfcc.shape[1],\n",
    "                                                 X_val_speakers_mfcc.shape[2],\n",
    "                                                 1)\n",
    "input_shape = (X_train_speakers_mfcc_nn.shape[1], X_train_speakers_mfcc_nn.shape[2], 1)\n",
    "enc, y_train_speakers_nn, target_names = data_preparation.transform_categorical_y(y_train_speakers)\n",
    "y_val_speakers_nn = enc.transform(y_val_speakers.reshape(-1, 1)).toarray()\n",
    "y_test_speakers_nn = enc.transform(y_test_speakers.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_speakers_best = np.concatenate([X_train_speakers_mfcc_nn, X_val_speakers_mfcc_nn])\n",
    "y_train_speakers_best = np.concatenate([y_train_speakers_nn, y_val_speakers_nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 39, 39, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 39, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 1,480,616\n",
      "Trainable params: 1,480,296\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 640 samples\n",
      "Epoch 1/21\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 1.0653 - accuracy: 0.6484\n",
      "Epoch 2/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.3942 - accuracy: 0.8984\n",
      "Epoch 3/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.3140 - accuracy: 0.9297\n",
      "Epoch 4/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.2330 - accuracy: 0.9563\n",
      "Epoch 5/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1858 - accuracy: 0.9734\n",
      "Epoch 6/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1570 - accuracy: 0.9812\n",
      "Epoch 7/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1478 - accuracy: 0.9828\n",
      "Epoch 8/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1181 - accuracy: 0.9906\n",
      "Epoch 9/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1309 - accuracy: 0.9859\n",
      "Epoch 10/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1108 - accuracy: 0.9875\n",
      "Epoch 11/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.1069 - accuracy: 0.9937\n",
      "Epoch 12/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0873 - accuracy: 0.9922\n",
      "Epoch 13/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0787 - accuracy: 0.9969\n",
      "Epoch 14/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0721 - accuracy: 0.9984\n",
      "Epoch 15/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0785 - accuracy: 0.9937\n",
      "Epoch 16/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0621 - accuracy: 0.9984\n",
      "Epoch 17/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0599 - accuracy: 0.9969\n",
      "Epoch 18/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0606 - accuracy: 1.0000\n",
      "Epoch 19/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0691 - accuracy: 0.9906\n",
      "Epoch 20/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0462 - accuracy: 0.9984\n",
      "Epoch 21/21\n",
      "640/640 [==============================] - 1s 2ms/sample - loss: 0.0533 - accuracy: 0.9984\n",
      "CPU times: user 35.4 s, sys: 8.67 s, total: 44.1 s\n",
      "Wall time: 27.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8277e34a90>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = cnn_models.simple_model(input_shape=input_shape, num_classes=8, batch_normalisation=True)\n",
    "model.fit(X_train_speakers_best, y_train_speakers_best,\n",
    "          batch_size=N_BATCH,\n",
    "          epochs=21,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_speakers_mfcc_nn = X_test_speakers_mfcc.reshape(X_test_speakers_mfcc.shape[0],\n",
    "                                                 X_test_speakers_mfcc.shape[1],\n",
    "                                                 X_test_speakers_mfcc.shape[2],\n",
    "                                                 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ale       0.91      1.00      0.95        20\n",
      "      alinda       0.77      1.00      0.87        20\n",
      "        gian       0.95      0.95      0.95        20\n",
      "     jackson       1.00      1.00      1.00       420\n",
      "      khaled       0.83      0.95      0.88        20\n",
      "     nicolas       0.99      1.00      1.00       420\n",
      "        theo       0.94      0.96      0.95       418\n",
      "    yweweler       0.98      0.93      0.95       420\n",
      "\n",
      "    accuracy                           0.97      1758\n",
      "   macro avg       0.92      0.97      0.94      1758\n",
      "weighted avg       0.97      0.97      0.97      1758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_nn = np.argmax(y_test_speakers_nn, axis=1)\n",
    "y_pred = model.predict_classes(X_test_speakers_mfcc_nn)\n",
    "print(classification_report(y_nn, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../best_models/speakers.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- [ ] Export train/val/test balanced split\n",
    "- [ ] Double check all the trials\n",
    "- [ ] Export in functions things like reshaping data for nn, evaluation blocks etc so that the notebook is more easy to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsim] *",
   "language": "python",
   "name": "conda-env-dsim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
